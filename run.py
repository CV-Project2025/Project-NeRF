import argparse
import csv
import itertools
import os

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import yaml

from src.core import NeuralField
from src.dataset import BlenderDataset
from src.renderer import render_image, render_rays
from src.utils import compute_psnr, compute_psnr_torch, render_image_safe, TensorBoardLogger


def run_part1(cfg, args):
    """Part 1: 2D å›¾åƒæ‹Ÿåˆ"""

    # å‚æ•°å¯¹æ¯”ç›¸å…³
    epochs = cfg["epochs"]
    learning_rate = cfg["learning_rate"]
    batch_size = cfg.get("batch_size", None)
    image_size = cfg.get("image_size", 400)
    log_dir = cfg.get("log_dir", "output/")
    
    # è·å–å›¾åƒåç§°ï¼ˆä¸å«æ‰©å±•åï¼‰å¹¶æ·»åŠ åˆ°è¾“å‡ºè·¯å¾„
    image_name = os.path.splitext(os.path.basename(args.image))[0]
    log_dir = os.path.join(log_dir, "part1", image_name)
    
    save_every = cfg.get("save_every", 500)
    output_dim = cfg["output_dim"]
    def ensure_list(value):
        if isinstance(value, (list, tuple)):
            return list(value)
        return [value]
    use_pe_list = ensure_list(cfg.get("use_positional_encoding", True))
    l_embed_list = ensure_list(cfg["L_embed"])
    hidden_dim_list = ensure_list(cfg["hidden_dim"])
    num_layers_list = ensure_list(cfg.get("num_layers", 3))
    param_combos = list(
        itertools.product(use_pe_list, l_embed_list, hidden_dim_list, num_layers_list)
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½å¹¶å¤„ç†2Då›¾åƒ
    img = Image.open(args.image).convert("RGB")
    w_orig, h_orig = img.size
    scale = min(image_size / w_orig, image_size / h_orig)
    new_w, new_h = int(w_orig * scale), int(h_orig * scale)
    img = img.resize((new_w, new_h), Image.LANCZOS)
    
    # ç”Ÿæˆå½’ä¸€åŒ–åæ ‡ç½‘æ ¼å’Œé¢œè‰²
    img_np = np.array(img) / 255.0
    h, w, _ = img_np.shape
    coords = torch.stack(
        torch.meshgrid(
            torch.linspace(0, 1, h), torch.linspace(0, 1, w), indexing="ij"
        ),
        dim=-1,
    ).reshape(-1, 2)
    gt_rgb = torch.tensor(img_np.reshape(-1, 3), dtype=torch.float32)
    coords, gt_rgb = coords.to(device), gt_rgb.to(device)

    os.makedirs(log_dir, exist_ok=True)
    results_path = os.path.join(log_dir, "final_psnr.csv")
    results_exists = os.path.exists(results_path)

    loss_fn = nn.MSELoss()

    total_pixels = coords.shape[0]
    print(">>> Start Training Part 1 (2D Fitting)...")
    print(
        f">>> å›¾åƒå°ºå¯¸: {h}x{w}, æ‰¹é‡å¤§å°: {'å…¨å›¾' if batch_size is None else batch_size}"
    )
    print(f">>> å‚æ•°ç»„åˆæ•°: {len(param_combos)}")

    with open(results_path, "a", newline="", encoding="utf-8") as f:
        fieldnames = [
            "use_positional_encoding",
            "L_embed",
            "hidden_dim",
            "num_layers",
            "epochs",
            "learning_rate",
            "batch_size",
            "image_size",
            "final_psnr",
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not results_exists:
            writer.writeheader()

        for run_idx, (use_pe, l_embed, hidden_dim, num_layers) in enumerate(
            param_combos, start=1
        ):
            config = {
                "mode": cfg["mode"],
                "L_embed": l_embed,
                "hidden_dim": hidden_dim,
                "output_dim": output_dim,
                "num_layers": num_layers,
                "use_positional_encoding": use_pe,
            }

            run_name = f"pe{int(bool(use_pe))}_L{l_embed}_H{hidden_dim}_N{num_layers}"
            run_dir = os.path.join(log_dir, run_name)
            os.makedirs(run_dir, exist_ok=True)

            save_intermediate = isinstance(save_every, int) and save_every > 0
            if save_intermediate:
                steps_dir = os.path.join(run_dir, "steps")
                os.makedirs(steps_dir, exist_ok=True)

            print(f">>> [{run_idx}/{len(param_combos)}] é…ç½®: {run_name}, Steps={epochs}")

            model = NeuralField(config).to(device)
            optimizer = optim.Adam(model.parameters(), lr=learning_rate)

            # è®­ç»ƒå¾ªç¯
            for i in tqdm(range(epochs)):
                if batch_size is None:
                    # å…¨å›¾è®­ç»ƒ
                    pred_rgb = model(coords)
                    loss = loss_fn(pred_rgb, gt_rgb)
                else:
                    # éšæœºæ‰¹é‡é‡‡æ ·
                    indices = torch.randint(0, total_pixels, (batch_size,), device=device)
                    batch_coords = coords[indices]
                    batch_gt_rgb = gt_rgb[indices]
                    pred_rgb = model(batch_coords)
                    loss = loss_fn(pred_rgb, batch_gt_rgb)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
                if save_intermediate and (i + 1) % save_every == 0:
                    with torch.no_grad():
                        intermediate_img = model(coords).cpu().numpy().reshape(h, w, 3)
                    plt.imsave(
                        os.path.join(steps_dir, f"step_{i+1:05d}.png"),
                        intermediate_img,
                    )

            # è®­ç»ƒå®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆç»“æœ
            with torch.no_grad():
                final_pred = model(coords)
                final_img = final_pred.cpu().numpy().reshape(h, w, 3)
                final_loss = loss_fn(final_pred, gt_rgb).item()

            final_img_path = os.path.join(run_dir, "final.png")
            plt.imsave(final_img_path, final_img)
            model_path = os.path.join(run_dir, "model_final.pth")
            torch.save(
                {"model_state_dict": model.state_dict(), "config": config},
                model_path,
            )

            final_psnr = compute_psnr(final_loss)
            writer.writerow(
                {
                    "use_positional_encoding": use_pe,
                    "L_embed": l_embed,
                    "hidden_dim": hidden_dim,
                    "num_layers": num_layers,
                    "epochs": epochs,
                    "learning_rate": learning_rate,
                    "batch_size": batch_size,
                    "image_size": image_size,
                    "final_psnr": final_psnr,
                }
            )
            f.flush()

            print(f">>> Done! Final PSNR: {final_psnr:.2f} dB")


def run_part2(cfg, args):
    """Part 2: NeRF 3Dåœºæ™¯é‡å»ºï¼Œè®­ç»ƒå’Œè¯„ä¼°"""
    if not args.data_dir:
        raise ValueError("Part 2 requires --data_dir pointing to a NeRF dataset root.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")

    # è¯»å–æ¸²æŸ“å’Œè®­ç»ƒé…ç½®
    downscale = cfg.get("downscale", 1)
    white_bkgd = cfg.get("white_bkgd", True)
    scene_scale = cfg.get("scene_scale", 1.0)
    near = float(cfg.get("near", 2.0))  # è¿‘å¹³é¢
    far = float(cfg.get("far", 6.0))  # è¿œå¹³é¢
    n_samples = cfg.get("n_samples", 64)  # è®­ç»ƒé‡‡æ ·ç‚¹æ•°
    render_n_samples = cfg.get("render_n_samples", n_samples)  # æ¸²æŸ“é‡‡æ ·ç‚¹æ•°
    batch_size = cfg.get("batch_size", 4096)  # æ¯æ‰¹å…‰çº¿æ•°
    train_iters = cfg.get("train_iters", 20000)  # è®­ç»ƒè¿­ä»£æ•°
    learning_rate = cfg.get("learning_rate", 5e-4)
    log_every = cfg.get("log_every", 100)
    save_every = cfg.get("save_every", 2000)
    chunk = cfg.get("chunk", 8192)  # æ¸²æŸ“å—å¤§å°
    log_dir = cfg.get("log_dir", "output/part2")
    if args.render_chunk:
        chunk = args.render_chunk

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    ckpt_dir = os.path.join(log_dir, "checkpoints")
    render_dir = os.path.join(log_dir, "renders")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(render_dir, exist_ok=True)

    # åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
    train_set = BlenderDataset(
        root_dir=args.data_dir,
        split="train",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    test_split = "test"
    test_meta = os.path.join(args.data_dir, "transforms_test.json")
    if not os.path.exists(test_meta):
        test_split = "val"
    test_set = BlenderDataset(
        root_dir=args.data_dir,
        split=test_split,
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )

    # åˆå§‹åŒ–æ¨¡å‹
    model = NeuralField(cfg).to(device)
    if args.checkpoint:
        ckpt = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        print(f">>> Loaded checkpoint: {args.checkpoint}")

    # è®­ç»ƒé˜¶æ®µ
    if not args.eval_only:
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        loss_fn = nn.MSELoss()

        print(">>> Start Training Part 2 (NeRF)...")
        model.train()
        for step in range(1, train_iters + 1):
            # éšæœºé‡‡æ ·å…‰çº¿å¹¶æ¸²æŸ“
            rays_o, rays_d, target = train_set.sample_random_rays(batch_size, device)
            pred_rgb, _, _ = render_rays(
                model=model,
                rays_o=rays_o,
                rays_d=rays_d,
                near=near,
                far=far,
                n_samples=n_samples,
                perturb=True,
                white_bkgd=white_bkgd,
            )
            loss = loss_fn(pred_rgb, target)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if step % log_every == 0:
                psnr = compute_psnr(loss.item())
                print(
                    f">>> Step {step}/{train_iters} | Loss {loss.item():.6f} | PSNR {psnr:.2f} dB"
                )

            if save_every and step % save_every == 0:
                ckpt_path = os.path.join(ckpt_dir, f"model_step_{step:06d}.pth")
                torch.save(
                    {"model_state_dict": model.state_dict(), "config": cfg}, ckpt_path
                )

        final_path = os.path.join(ckpt_dir, "model_final.pth")
        torch.save({"model_state_dict": model.state_dict(), "config": cfg}, final_path)

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # è¯„ä¼°é˜¶æ®µï¼šæ¸²æŸ“æµ‹è¯•é›†
    model.eval()
    print(f">>> Rendering {test_split} set...")
    psnrs = []
    with torch.no_grad():
        for idx in range(len(test_set)):
            rays_o, rays_d, target = test_set.get_image_rays(idx, device)
            pred = render_image_safe(
                render_fn=render_image,
                model=model,
                rays_o=rays_o,
                rays_d=rays_d,
                near=near,
                far=far,
                n_samples=render_n_samples,
                chunk=chunk,
                white_bkgd=white_bkgd,
            )
            pred = torch.clamp(pred, 0.0, 1.0)
            psnr = compute_psnr_torch(pred, target)
            psnrs.append(psnr)
            plt.imsave(
                os.path.join(render_dir, f"test_{idx:03d}.png"),
                pred.cpu().numpy(),
            )

    avg_psnr = float(np.mean(psnrs)) if psnrs else 0.0
    print(f">>> Test PSNR: {avg_psnr:.2f} dB")
    print(f">>> Rendered images saved to: {render_dir}")


def run_part2_instant(cfg, args):
    """Part 2 Instant: Instant-NeRF åŠ é€Ÿè®­ç»ƒ"""
    if not args.data_dir:
        raise ValueError("Part 2 Instant requires --data_dir pointing to a NeRF dataset root.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type != 'cuda':
        print("!!! Instant-NeRF åœ¨ CPU ä¸Šæ€§èƒ½æ— æ³•å‘æŒ¥ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨ CUDA GPU")

    # è¯»å–æ¸²æŸ“å’Œè®­ç»ƒé…ç½®
    downscale = cfg.get("downscale", 2)
    white_bkgd = cfg.get("white_bkgd", True)
    scene_scale = cfg.get("scene_scale", 1.0)
    near = float(cfg.get("near", 2.0))
    far = float(cfg.get("far", 6.0))
    n_samples = cfg.get("n_samples", 32)  # Instant-NeRF éœ€è¦æ›´å°‘é‡‡æ ·ç‚¹
    render_n_samples = cfg.get("render_n_samples", n_samples)
    batch_size = cfg.get("batch_size", 8192)  # Instant-NeRF ä½¿ç”¨æ›´å¤§æ‰¹é‡
    train_iters = cfg.get("train_iters", 5000)  # Instant-NeRF è®­ç»ƒæ›´å¿«
    learning_rate = cfg.get("learning_rate", 0.01)  # Instant-NeRF ä½¿ç”¨é«˜å­¦ä¹ ç‡
    log_every = cfg.get("log_every", 50)
    save_every = cfg.get("save_every", 500)
    chunk = cfg.get("chunk", 16384)  # æ›´å¤§çš„æ¸²æŸ“å—
    log_dir = cfg.get("log_dir", "output/part2_instant")
    
    # è·å–æ•°æ®é›†åç§°å¹¶æ·»åŠ åˆ°è¾“å‡ºè·¯å¾„
    dataset_name = os.path.basename(args.data_dir)
    log_dir = os.path.join(log_dir, dataset_name)
    
    if args.render_chunk:
        chunk = args.render_chunk

    # Instant-NeRF ç‰¹æœ‰é…ç½®
    use_density_grid = cfg.get("use_density_grid", True)
    grid_resolution = cfg.get("grid_resolution", 128)
    grid_threshold = cfg.get("grid_threshold", 0.01)
    grid_update_interval = cfg.get("grid_update_interval", 16)
    grid_warmup_iters = cfg.get("grid_warmup_iters", 256)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    ckpt_dir = os.path.join(log_dir)
    render_dir = os.path.join(log_dir, "renders")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(render_dir, exist_ok=True)

    # åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
    train_set = BlenderDataset(
        root_dir=args.data_dir,
        split="train",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    # åŠ è½½æµ‹è¯•é›†
    test_split = "test"
    test_meta = os.path.join(args.data_dir, "transforms_test.json")
    if not os.path.exists(test_meta):
        test_split = "val"
    test_set = BlenderDataset(
        root_dir=args.data_dir,
        split=test_split,
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )

    
    # åªåœ¨è®­ç»ƒæ¨¡å¼ä¸‹åˆ’åˆ†éªŒè¯é›†
    if not args.eval_only:
        # ä»æµ‹è¯•é›†ä¸­éšæœºæŠ½å–30%ä½œä¸ºéªŒè¯é›†
        import random
        n_test = len(test_set.images)
        n_val = int(n_test * 0.3)
        val_indices = random.sample(range(n_test), n_val)
        test_indices = [i for i in range(n_test) if i not in val_indices]
        
        # åˆ›å»ºéªŒè¯é›†
        val_set = BlenderDataset(
            root_dir=args.data_dir,
            split=test_split,
            downscale=downscale,
            white_bkgd=white_bkgd,
            scene_scale=scene_scale,
        )
        val_set.images = test_set.images[val_indices]
        val_set.poses = test_set.poses[val_indices]
        
        # ä¸ç¼©å‡æµ‹è¯•é›†ï¼Œä¿æŒå®Œæ•´
        print(f">>> æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {len(train_set.images)} å¼  | éªŒè¯é›† {len(val_set.images)} å¼  | æµ‹è¯•é›† {len(test_set.images)} å¼ ")
    else:
        # è¯„ä¼°æ¨¡å¼ï¼šä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†
        print(f">>> è¯„ä¼°ä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›† {len(test_set.images)} å¼ ")
        val_set = None

    # åˆå§‹åŒ–æ¨¡å‹
    print(">>> åˆå§‹åŒ– Instant-NeRF æ¨¡å‹...")
    model = NeuralField(cfg).to(device)
    

    # è‡ªåŠ¨æ£€æµ‹ scene_boundï¼ˆå¦‚æœé…ç½®ä¸­è®¾ç½®ä¸º "auto"ï¼‰
    if cfg.get('scene_bound') == 'auto':
        # ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†å§¿æ€ä¸­æå–ç›¸æœºä½ç½®
        all_poses = torch.cat([train_set.poses, test_set.poses], dim=0)
        cam_positions = all_poses[:, :3, 3].cpu().numpy()
        
        # è®¡ç®—ç›¸æœºåˆ°åŸç‚¹çš„æœ€å¤§è·ç¦»
        max_distance = np.max(np.linalg.norm(cam_positions, axis=1))
        
        # æ·»åŠ 5%ä½™é‡ä½œä¸º scene_bound
        scene_bound_auto = max_distance * 1.05
        cfg['scene_bound'] = scene_bound_auto
        print(f">>> è‡ªåŠ¨æ£€æµ‹ scene_bound: {scene_bound_auto:.2f}ï¼ˆåŸºäºç›¸æœºæœ€å¤§è·ç¦» {max_distance:.2f}ï¼‰")


    # åˆå§‹åŒ–å æ®ç½‘æ ¼
    density_grid = None
    active_ratio = 1.0  # åˆå§‹åŒ–æ´»è·ƒæ¯”ä¾‹ï¼ˆwarmup æœŸé—´é»˜è®¤ 100%ï¼‰
    if use_density_grid:
        from src.renderer import DensityGrid
        density_grid = DensityGrid(
            resolution=grid_resolution,
            bound=cfg.get('scene_bound', 1.5),
            threshold=grid_threshold
        ).to(device)
        print(f">>> Density Grid å·²å¯ç”¨: {grid_resolution}Â³ åˆ†è¾¨ç‡")
    else:
        print(">>> Density Grid å·²ç¦ç”¨ï¼ˆæ€§èƒ½ä¼šé™ä½ï¼‰")
    
    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆåŒ…æ‹¬ density_gridï¼‰
    if args.checkpoint:
        ckpt = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        if density_grid is not None and "density_grid" in ckpt:
            density_grid.load_state_dict(ckpt["density_grid"])
            print(f">>> Loaded checkpoint with DensityGrid: {args.checkpoint} (Step {ckpt.get("step", "æœªçŸ¥")} | Val PSNR {ckpt.get("val_psnr", None):.2f} dB)")
        else:
            print(f">>> Loaded checkpoint: {args.checkpoint} (Step {ckpt.get("step", "æœªçŸ¥")} | Val PSNR {ckpt.get("val_psnr", None):.2f} dB)")

    # è®­ç»ƒé˜¶æ®µ
    if not args.eval_only:
        # åˆå§‹åŒ– TensorBoard
        tb_dir = os.path.join(log_dir, "tensorboard")
        tb_logger = TensorBoardLogger(tb_dir)
        
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        loss_fn = nn.MSELoss()

        print(f">>> ç›®æ ‡: {train_iters} æ­¥")
        print(f">>> å­¦ä¹ ç‡: {learning_rate} ")
        print(f">>> æ‰¹é‡å¤§å°: {batch_size}")
        print(f">>> é‡‡æ ·ç‚¹æ•°: {n_samples} ")
        print(f">>>  tensorboard --logdir={tb_dir} æŸ¥çœ‹ TensorBoard æ—¥å¿—")
        
        # åˆå§‹åŒ–æœ€ä½³éªŒè¯é›†PSNRè·Ÿè¸ª
        best_val_psnr = 0.0
        
        model.train()
        for step in range(1, train_iters + 1):
            # éšæœºé‡‡æ ·å…‰çº¿å¹¶æ¸²æŸ“
            rays_o, rays_d, target = train_set.sample_random_rays(batch_size, device)
            
            # ä½¿ç”¨å æ®ç½‘æ ¼åŠ é€Ÿæ¸²æŸ“
            pred_rgb, _, _ = render_rays(
                model=model,
                rays_o=rays_o,
                rays_d=rays_d,
                near=near,
                far=far,
                n_samples=n_samples,
                perturb=True,
                white_bkgd=white_bkgd,
                density_grid=density_grid,  
            )
            loss = loss_fn(pred_rgb, target)

            optimizer.zero_grad()
            loss.backward()
            
            # åˆ†åˆ«è£å‰ªæ•£åˆ—è¡¨å’Œ MLP çš„æ¢¯åº¦
            if hasattr(model, 'representation'):
                torch.nn.utils.clip_grad_norm_(model.representation.parameters(), max_norm=1.0)
            if hasattr(model, 'decoder'):
                torch.nn.utils.clip_grad_norm_(model.decoder.parameters(), max_norm=1.0)
            
            optimizer.step()

            # å®šæœŸæ›´æ–° Density Gridï¼ˆwarmup åæ‰å¼€å§‹ï¼‰
            if density_grid is not None and density_grid.should_update(step, grid_update_interval, grid_warmup_iters):
                model.eval()
                active_ratio = density_grid.update(model, device=device, time=None)
                model.train()

            # æ—¥å¿—è¾“å‡ºå’Œ TensorBoard è®°å½•
            if step % log_every == 0:
                psnr = compute_psnr(loss.item())
                skip_info = ""
                if density_grid is not None:
                    skip_info = f" | Skip: {(1-active_ratio)*100:.1f}%"
                print(
                    f">>> Step {step}/{train_iters} | Loss {loss.item():.6f} | PSNR {psnr:.2f} dB{skip_info}"
                )
                
                # è®°å½•åˆ° TensorBoard
                tb_logger.log_scalar('Train/Loss', loss.item(), step)
                tb_logger.log_scalar('Train/PSNR', psnr, step)
                if density_grid is not None:
                    tb_logger.log_scalar('Train/ActiveRatio', active_ratio, step)
            
            # å®šæœŸéªŒè¯é›†è¯„ä¼°
            val_every = cfg.get("val_every", 500)
            if step % val_every == 0:
                model.eval()
                val_psnrs = []
                with torch.no_grad():
                    for idx in range(len(val_set.images)):
                        rays_o, rays_d, target = val_set.get_image_rays(idx, device)
                        rays_o = rays_o.reshape(-1, 3)
                        rays_d = rays_d.reshape(-1, 3)
                        target = target.reshape(-1, 3)
                        
                        # åˆ†å—æ¸²æŸ“éªŒè¯é›†
                        pred_chunks = []
                        for i in range(0, rays_o.shape[0], chunk):
                            pred_chunk, _, _ = render_rays(
                                model=model,
                                rays_o=rays_o[i:i+chunk],
                                rays_d=rays_d[i:i+chunk],
                                near=near,
                                far=far,
                                n_samples=render_n_samples,
                                perturb=False,
                                white_bkgd=white_bkgd,
                                density_grid=density_grid,
                            )
                            pred_chunks.append(pred_chunk)
                        pred = torch.cat(pred_chunks, dim=0)
                        val_psnr = compute_psnr_torch(pred, target)
                        val_psnrs.append(val_psnr)
                
                avg_val_psnr = float(np.mean(val_psnrs))
                print(f"    [Validation] PSNR: {avg_val_psnr:.2f} dB", end="")
                
                # è®°å½•éªŒè¯é›† PSNR åˆ° TensorBoard
                tb_logger.log_scalar('Validation/PSNR', avg_val_psnr, step)
                
                # åªåœ¨éªŒè¯é›†PSNRæå‡æ—¶ä¿å­˜æ¨¡å‹
                if avg_val_psnr > best_val_psnr:
                    best_val_psnr = avg_val_psnr
                    best_path = os.path.join(ckpt_dir, f"best_model.pth")
                    save_dict = {
                        "model_state_dict": model.state_dict(),
                        "config": cfg,
                        "step": step,
                        "val_psnr": best_val_psnr
                    }
                    if density_grid is not None:
                        save_dict["density_grid"] = density_grid.state_dict()
                    torch.save(save_dict, best_path)
                    print(f" | ğŸŒŸ New Best Model! Saved to {best_path}")
                else:
                    print()
                
                model.train()

        print(f"\n>>> è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›† PSNR: {best_val_psnr:.2f} dB")
        tb_logger.close()

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # è¯„ä¼°é˜¶æ®µ
    if args.eval_only:
        import random
        import shutil
        import subprocess
        
        # åˆ¤æ–­æ˜¯å¦é¡ºåºæ¸²æŸ“å¹¶ç”Ÿæˆè§†é¢‘ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨æµ‹è¯•é›†ï¼‰
        if args.render_n == -1:
            n_render = len(test_set.images)
            render_indices = list(range(n_render))
            
            # åˆ›å»ºä¸´æ—¶å›¾ç‰‡ç›®å½•å’Œè§†é¢‘ç›®å½•
            picture_dir = os.path.join(log_dir, "picture")
            video_dir = os.path.join(log_dir)
            os.makedirs(picture_dir, exist_ok=True)
            os.makedirs(video_dir, exist_ok=True)
            
            print(f"\n>>> æ¸²æŸ“å…¨éƒ¨æµ‹è¯•é›†å›¾ç‰‡ï¼ˆæŒ‰é¡ºåº {n_render} å¼ ï¼‰ç”¨äºç”Ÿæˆè§†é¢‘...")
            
            model.eval()
            psnrs = []
            with torch.no_grad():
                for i, idx in enumerate(tqdm(render_indices)):
                    rays_o, rays_d, target = test_set.get_image_rays(idx, device)
                    H, W = rays_o.shape[:2]
                    rays_o = rays_o.reshape(-1, 3)
                    rays_d = rays_d.reshape(-1, 3)
                    
                    # ä½¿ç”¨ density_grid åŠ é€Ÿæ¸²æŸ“
                    pred_chunks = []
                    for j in range(0, rays_o.shape[0], chunk):
                        pred_chunk, _, _ = render_rays(
                            model=model,
                            rays_o=rays_o[j:j+chunk],
                            rays_d=rays_d[j:j+chunk],
                            near=near,
                            far=far,
                            n_samples=render_n_samples,
                            perturb=False,
                            white_bkgd=white_bkgd,
                            density_grid=density_grid,  # ä½¿ç”¨å æ®ç½‘æ ¼åŠ é€Ÿ
                        )
                        pred_chunks.append(pred_chunk)
                    
                    pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                    pred = torch.clamp(pred, 0.0, 1.0)
                    psnr = compute_psnr_torch(pred, target)
                    psnrs.append(psnr)
                    
                    # ä¿å­˜ä¸ºè¿ç»­ç¼–å·çš„å¸§
                    plt.imsave(
                        os.path.join(picture_dir, f"frame_{i:03d}.png"),
                        pred.cpu().numpy(),
                    )
            
            avg_psnr = float(np.mean(psnrs))
            print(f"\n>>> æ¸²æŸ“å®Œæˆï¼å¹³å‡ PSNR: {avg_psnr:.2f} dB")
            
            # ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘
            dataset_name = os.path.basename(args.data_dir)
            video_path = os.path.join(video_dir, f"{dataset_name}_24fps.mp4")
            print(f"\n>>> ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘...")
            try:
                cmd = [
                    "ffmpeg", "-y",
                    "-framerate", "24",
                    "-i", os.path.join(picture_dir, "frame_%03d.png"),
                    "-c:v", "libx264",
                    "-pix_fmt", "yuv420p",
                    "-crf", "18",
                    video_path
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    print(f">>> è§†é¢‘å·²ä¿å­˜: {video_path}")
                    print(f">>> è§†é¢‘æ—¶é•¿: {n_render/24:.1f}ç§’ ({n_render} å¸§ @ 24fps)")
                    
                    # åˆ é™¤ä¸´æ—¶å›¾ç‰‡ç›®å½•
                    shutil.rmtree(picture_dir)
                    print(f">>> å·²æ¸…ç†ä¸´æ—¶å›¾ç‰‡ç›®å½•")
                else:
                    print(f"!!! ffmpeg æ‰§è¡Œå¤±è´¥:\n{result.stderr}")
            except FileNotFoundError:
                print("!!! æœªæ‰¾åˆ° ffmpegï¼Œè¯·å…ˆå®‰è£…: sudo apt install ffmpeg")
            except Exception as e:
                print(f"!!! è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
        else:
            # éšæœºæ¸²æŸ“æŒ‡å®šæ•°é‡çš„å›¾ç‰‡
            n_render = min(args.render_n, len(test_set.images))
            render_indices = random.sample(range(len(test_set.images)), n_render)
            
            print(f"\n>>> æ¸²æŸ“æµ‹è¯•é›†å›¾ç‰‡ï¼ˆéšæœº {n_render} å¼ ï¼‰...")
            os.makedirs(render_dir, exist_ok=True)
            psnrs = []
            
            model.eval()
            with torch.no_grad():
                for i, idx in enumerate(tqdm(render_indices)):
                    rays_o, rays_d, target = test_set.get_image_rays(idx, device)
                    H, W = rays_o.shape[:2]
                    rays_o = rays_o.reshape(-1, 3)
                    rays_d = rays_d.reshape(-1, 3)
                    
                    # ä½¿ç”¨ density_grid åŠ é€Ÿæ¸²æŸ“
                    pred_chunks = []
                    for j in range(0, rays_o.shape[0], chunk):
                        pred_chunk, _, _ = render_rays(
                            model=model,
                            rays_o=rays_o[j:j+chunk],
                            rays_d=rays_d[j:j+chunk],
                            near=near,
                            far=far,
                            n_samples=render_n_samples,
                            perturb=False,
                            white_bkgd=white_bkgd,
                            density_grid=density_grid,  
                        )
                        pred_chunks.append(pred_chunk)
                    
                    pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                    pred = torch.clamp(pred, 0.0, 1.0)
                    psnr = compute_psnr_torch(pred, target)
                    psnrs.append(psnr)
                    
                    # ä¿å­˜æ¸²æŸ“å›¾ç‰‡ï¼ˆå¸¦PSNRä¿¡æ¯ï¼‰
                    plt.imsave(
                        os.path.join(render_dir, f"render_{idx:03d}_psnr{psnr:.2f}.png"),
                        pred.cpu().numpy(),
                    )
            
            avg_psnr = float(np.mean(psnrs))
            print(f"\n>>> æ¸²æŸ“å®Œæˆï¼å¹³å‡ PSNR: {avg_psnr:.2f} dB")
            print(f">>> ä¿å­˜è·¯å¾„: {render_dir}")
        return
    
    # è®­ç»ƒåçš„æ ‡å‡†è¯„ä¼°ï¼šè®¡ç®—æµ‹è¯•é›†PSNR
    model.eval()
    print(f"\n>>> è¯„ä¼° {test_split} é›†...")
    psnrs = []
    with torch.no_grad():
        for idx in tqdm(range(len(test_set))):
            rays_o, rays_d, target = test_set.get_image_rays(idx, device)
            H, W = rays_o.shape[:2]
            rays_o = rays_o.reshape(-1, 3)
            rays_d = rays_d.reshape(-1, 3)
            
            # ä½¿ç”¨ density_grid åŠ é€Ÿæ¸²æŸ“
            pred_chunks = []
            for j in range(0, rays_o.shape[0], chunk):
                pred_chunk, _, _ = render_rays(
                    model=model,
                    rays_o=rays_o[j:j+chunk],
                    rays_d=rays_d[j:j+chunk],
                    near=near,
                    far=far,
                    n_samples=render_n_samples,
                    perturb=False,
                    white_bkgd=white_bkgd,
                    density_grid=density_grid,
                )
                pred_chunks.append(pred_chunk)
            
            pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
            pred = torch.clamp(pred, 0.0, 1.0)
            psnr = compute_psnr_torch(pred, target)
            psnrs.append(psnr)

    avg_psnr = float(np.mean(psnrs)) if psnrs else 0.0
    print(f"\n{'='*60}")
    print(f">>> Instant-NeRF è¯„ä¼°ç»“æœ")
    print(f">>> æµ‹è¯•é›†å¹³å‡ PSNR: {avg_psnr:.2f} dB")
    print(f">>> æœ€ä½³éªŒè¯é›† PSNR: {best_val_psnr:.2f} dB" if not args.eval_only else "")
    print(f"{'='*60}")


def run_part3(cfg, args):
    """Part 3: åŠ¨æ€ NeRF"""
    if not args.data_dir:
        raise ValueError("Part 3 requires --data_dir pointing to a dynamic NeRF dataset root.")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")

    # è¯»å–æ¸²æŸ“å’Œè®­ç»ƒé…ç½®
    downscale = cfg.get("downscale", 1)
    white_bkgd = cfg.get("white_bkgd", True)
    scene_scale = cfg.get("scene_scale", 1.0)
    near = float(cfg.get("near", 2.0))
    far = float(cfg.get("far", 6.0))
    n_samples = cfg.get("n_samples", 64)
    render_n_samples = cfg.get("render_n_samples", n_samples)
    batch_size = cfg.get("batch_size", 4096)
    train_iters = cfg.get("train_iters", 20000)
    learning_rate = cfg.get("learning_rate", 5e-4)
    log_every = cfg.get("log_every", 100)
    save_every = cfg.get("save_every", 2000)
    chunk = cfg.get("chunk", 8192)
    deformation_reg_weight = cfg.get("deformation_reg_weight", 1e-4) # å˜å½¢æ­£åˆ™åŒ–æƒé‡
    render_n = args.render_n
    if args.render_chunk is not None:
        chunk = args.render_chunk
    log_dir = cfg.get("log_dir", "output/part3")

    os.makedirs(log_dir, exist_ok=True)
    ckpt_dir = os.path.join(log_dir, "checkpoints")
    render_dir = os.path.join(log_dir, "renders")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(render_dir, exist_ok=True)

    from src.dataset import DynamicDataset
    
    train_set = DynamicDataset(
        root_dir=args.data_dir,
        split="train",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    # åŠ è½½æµ‹è¯•é›†
    test_split = "test"
    test_meta = os.path.join(args.data_dir, "transforms_test.json")
    if not os.path.exists(test_meta):
        test_split = "val"
    test_set = DynamicDataset(
        root_dir=args.data_dir,
        split=test_split,
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )

    # æ¨¡å‹åˆå§‹åŒ–
    from src.core import NeuralField
    model = NeuralField(cfg).to(device)
    
    # å¦‚æœä½¿ç”¨ instant æ¨¡å¼ï¼Œå¯ç”¨ density_grid
    canonical_type = cfg.get('canonical_type', 'nerf')
    density_grid = None
    active_ratio = 1.0
    if canonical_type == 'instant':
        use_density_grid = cfg.get('use_density_grid', True)
        if use_density_grid:
            from src.renderer import DensityGrid
            grid_resolution = cfg.get('grid_resolution', 128)
            grid_threshold = cfg.get('grid_threshold', 0.01)
            scene_bound = cfg.get('scene_bound', 1.5)
            density_grid = DensityGrid(
                resolution=grid_resolution,
                bound=scene_bound,
                threshold=grid_threshold
            ).to(device)
            print(f">>> Density Grid å·²å¯ç”¨: {grid_resolution}Â³ åˆ†è¾¨ç‡ (Instant-NGP æ¨¡å¼)")
    
    if args.checkpoint:
        ckpt = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        if density_grid is not None and "density_grid" in ckpt:
            density_grid.load_state_dict(ckpt["density_grid"])
        print(f">>> Loaded checkpoint: {args.checkpoint}")

    # è®­ç»ƒé˜¶æ®µ
    if not args.eval_only:
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        loss_fn = nn.MSELoss()
        print(">>> å¼€å§‹è®­ç»ƒ Part 3 (Dynamic NeRF)...")

        model.train()
        grid_update_interval = cfg.get('grid_update_interval', 16)
        grid_warmup_iters = cfg.get('grid_warmup_iters', 256)
        
        for step in range(1, train_iters + 1):
            rays_o, rays_d, target, times = train_set.sample_random_rays(batch_size, device)
            
            # è°ƒç”¨ä¿®æ”¹åçš„ render_raysï¼Œæ¥æ”¶ extras
            pred_rgb, _, _, extras = render_rays(
                model=model,
                rays_o=rays_o,
                rays_d=rays_d,
                near=near,
                far=far,
                n_samples=n_samples,
                perturb=True,
                white_bkgd=white_bkgd,
                times=times,
                density_grid=density_grid,
            )
            
            # A. è¾…åŠ©æŸå¤±å‡½æ•°: RGB Loss + Deformation Regularization
            loss_rgb = loss_fn(pred_rgb, target)
            mean_delta_x = extras['mean_delta_x'] # ä» extras è·å–åŠ æƒå¹³å‡å˜å½¢é‡
            loss_reg = torch.mean(mean_delta_x ** 2) * deformation_reg_weight
            total_loss = loss_rgb + loss_reg

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            if density_grid is not None and density_grid.should_update(step, grid_update_interval, grid_warmup_iters):
                model.eval()
                # éšæœºé‡‡æ ·ä¸€ä¸ªæ—¶åˆ»è¿›è¡Œå¢é‡æ›´æ–°,å¤šæ¬¡è¿­ä»£åä¼šè‡ªåŠ¨å½¢æˆè¿åŠ¨è½¨è¿¹çš„æ—¶ç©ºå¹¶é›†
                time_min = train_set.times.min().item()
                time_max = train_set.times.max().item()
                rand_time = torch.rand(1, 1, device=device) * (time_max - time_min) + time_min
                active_ratio = density_grid.update(model, device=device, time=rand_time, decay=0.95)
                model.train()

            if step % log_every == 0:
                psnr = compute_psnr(loss_rgb.item())
                skip_info = ""
                if density_grid is not None:
                    skip_info = f" | Skip: {(1-active_ratio)*100:.1f}%"
                print(
                    f">>> Step {step}/{train_iters} | "
                    f"RGB Loss {loss_rgb.item():.6f} | "
                    f"Reg Loss {loss_reg.item():.9f} | "
                    f"PSNR {psnr:.2f} dB{skip_info}"
                )

            if save_every and step % save_every == 0:
                ckpt_path = os.path.join(ckpt_dir, f"model_step_{step:06d}.pth")
                save_dict = {"model_state_dict": model.state_dict(), "config": cfg}
                if density_grid is not None:
                    save_dict["density_grid"] = density_grid.state_dict()
                torch.save(save_dict, ckpt_path)

        final_path = os.path.join(ckpt_dir, "model_final.pth")
        save_dict = {"model_state_dict": model.state_dict(), "config": cfg}
        if density_grid is not None:
            save_dict["density_grid"] = density_grid.state_dict()
        torch.save(save_dict, final_path)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # è¯„ä¼°é˜¶æ®µ
    model.eval()
    print(f">>> Rendering {test_split} set...")
    psnrs = []
    
    with torch.no_grad():
        num_renders = len(test_set) if render_n == -1 else min(render_n, len(test_set))
        for idx in range(num_renders):
            rays_o, rays_d, target, time = test_set.get_image_rays(idx, device)
            H, W = rays_o.shape[:2]
            rays_o = rays_o.reshape(-1, 3)
            rays_d = rays_d.reshape(-1, 3)
            time = time.expand(H*W, 1)

            pred_chunks = []
            for i in range(0, rays_o.shape[0], chunk):
                pred_chunk, _, _, _ = render_rays(
                    model=model,
                    rays_o=rays_o[i:i+chunk],
                    rays_d=rays_d[i:i+chunk],
                    near=near,
                    far=far,
                    n_samples=render_n_samples,
                    perturb=False,
                    white_bkgd=white_bkgd,
                    times=time[i:i+chunk],
                    density_grid=density_grid,
                )
                pred_chunks.append(pred_chunk)
            
            pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
            pred = torch.clamp(pred, 0.0, 1.0)
            psnr = compute_psnr_torch(pred, target)
            psnrs.append(psnr)
            
            plt.imsave(
                os.path.join(render_dir, f"{test_split}_{idx:03d}_t{time[0,0]:.2f}.png"),
                pred.cpu().numpy(),
            )

    avg_psnr = float(np.mean(psnrs)) if psnrs else 0.0
    print(f">>> Test PSNR: {avg_psnr:.2f} dB")
    print(f">>> Rendered images saved to: {render_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--image", type=str, help="è¾“å…¥å›¾åƒè·¯å¾„ (Part 1)")
    parser.add_argument("--data_dir", type=str, help="NeRF æ•°æ®é›†æ ¹ç›®å½• (Part 2)")
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", type=str, help="åŠ è½½ Part 2 å·²è®­ç»ƒæ¨¡å‹")
    parser.add_argument(
        "--eval_only",
        action="store_true",
        help="ä»…æ¸²æŸ“æµ‹è¯•é›†ï¼Œä¸è¿›è¡Œè®­ç»ƒï¼ˆéœ€ --checkpointï¼‰",
    )
    parser.add_argument("--render_n", type=int, default=10, help="è¯„ä¼°æ—¶æ¸²æŸ“çš„æµ‹è¯•é›†å›¾ç‰‡æ•°é‡ï¼Œå¦‚æœä¸º -1 åˆ™æ¸²æŸ“å…¨éƒ¨") 
    parser.add_argument("--render_chunk", type=int, help="è¦†ç›–æ¸²æŸ“ chunk å¤§å°")
    args = parser.parse_args()

    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    mode = cfg.get("mode")
    if mode == "part1_fourier":
        if not args.image:
            raise ValueError("Part 1 requires --image.")
        run_part1(cfg, args)
    elif mode == "part2_nerf":
        if args.eval_only and not args.checkpoint:
            raise ValueError("eval_only requires --checkpoint.")
        run_part2(cfg, args)
    elif mode == "part2_instant":
        if args.eval_only and not args.checkpoint:
            raise ValueError("eval_only requires --checkpoint.")
        run_part2_instant(cfg, args)
    elif mode == "part3":
        if args.eval_only and not args.checkpoint:
            raise ValueError("eval_only requires --checkpoint.")
        run_part3(cfg, args)
    else:
        raise ValueError(f"Unsupported mode: {mode}")
