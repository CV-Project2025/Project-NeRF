import argparse
import csv
import itertools
import os

import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import yaml

from src.core import NeuralField
from src.dataset import BlenderDataset
from src.renderer import render_image, render_rays
from src.utils import compute_psnr, compute_psnr_torch, render_image_safe, TensorBoardLogger, get_exp_name


def run_part1(cfg, args):
    """Part 1: 2D å›¾åƒæ‹Ÿåˆ"""

    # å‚æ•°å¯¹æ¯”ç›¸å…³
    epochs = cfg["epochs"]
    learning_rate = cfg["learning_rate"]
    batch_size = cfg.get("batch_size", None)
    image_size = cfg.get("image_size", 400)
    log_dir = cfg.get("log_dir", "output/")
    
    # è·å–å›¾åƒåç§°ï¼ˆä¸å«æ‰©å±•åï¼‰å¹¶æ·»åŠ åˆ°è¾“å‡ºè·¯å¾„
    image_name = os.path.splitext(os.path.basename(args.image))[0]
    log_dir = os.path.join(log_dir, "part1", image_name)
    
    save_every = cfg.get("save_every", 500)
    log_every = cfg.get("log_every", 100)  # æ—¥å¿—è®°å½•é¢‘ç‡
    output_dim = cfg["output_dim"]
    def ensure_list(value):
        if isinstance(value, (list, tuple)):
            return list(value)
        return [value]
    use_pe_list = ensure_list(cfg.get("use_positional_encoding", True))
    l_embed_list = ensure_list(cfg["L_embed"])
    hidden_dim_list = ensure_list(cfg["hidden_dim"])
    num_layers_list = ensure_list(cfg.get("num_layers", 3))
    param_combos = list(
        itertools.product(use_pe_list, l_embed_list, hidden_dim_list, num_layers_list)
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½å¹¶å¤„ç†2Då›¾åƒ
    img = Image.open(args.image).convert("RGB")
    w_orig, h_orig = img.size
    scale = min(image_size / w_orig, image_size / h_orig)
    new_w, new_h = int(w_orig * scale), int(h_orig * scale)
    img = img.resize((new_w, new_h), Image.LANCZOS)
    
    # ç”Ÿæˆå½’ä¸€åŒ–åæ ‡ç½‘æ ¼å’Œé¢œè‰²
    img_np = np.array(img) / 255.0
    h, w, _ = img_np.shape
    coords = torch.stack(
        torch.meshgrid(
            torch.linspace(0, 1, h), torch.linspace(0, 1, w), indexing="ij"
        ),
        dim=-1,
    ).reshape(-1, 2)
    gt_rgb = torch.tensor(img_np.reshape(-1, 3), dtype=torch.float32)
    coords, gt_rgb = coords.to(device), gt_rgb.to(device)

    os.makedirs(log_dir, exist_ok=True)
    results_path = os.path.join(log_dir, "final_psnr.csv")
    results_exists = os.path.exists(results_path)

    loss_fn = nn.MSELoss()

    if args.eval_only:
        ckpt = torch.load(args.checkpoint, map_location=device)
        ckpt_cfg = ckpt.get("config", cfg)
        model = NeuralField(ckpt_cfg).to(device)
        load_result = model.load_state_dict(ckpt["model_state_dict"], strict=False)
        if load_result.missing_keys or load_result.unexpected_keys:
            print(f">>> Warning: load_state_dict missing={load_result.missing_keys}, "
                  f"unexpected={load_result.unexpected_keys}")
        model.eval()
        with torch.no_grad():
            pred = model(coords)
            pred = torch.clamp(pred, 0.0, 1.0)
            loss = loss_fn(pred, gt_rgb).item()
            psnr = compute_psnr(loss)
            final_img = pred.cpu().numpy().reshape(h, w, 3)

        eval_dir = os.path.join(log_dir, "eval")
        os.makedirs(eval_dir, exist_ok=True)
        ckpt_name = os.path.splitext(os.path.basename(args.checkpoint))[0]
        out_path = os.path.join(eval_dir, f"{ckpt_name}.png")
        plt.imsave(out_path, final_img)
        print(f">>> Eval PSNR: {psnr:.2f} dB")
        print(f">>> Rendered image saved to: {out_path}")
        return

    total_pixels = coords.shape[0]
    print(">>> Start Training Part 1 (2D Fitting)...")
    print(
        f">>> å›¾åƒå°ºå¯¸: {h}x{w}, æ‰¹é‡å¤§å°: {'å…¨å›¾' if batch_size is None else batch_size}"
    )
    print(f">>> å‚æ•°ç»„åˆæ•°: {len(param_combos)}")

    # åˆå§‹åŒ– TensorBoard
    tb_base_dir = os.path.join(log_dir, "tensorboard")
    os.makedirs(tb_base_dir, exist_ok=True)
    print(f">>> tensorboard --logdir={tb_base_dir} æŸ¥çœ‹ TensorBoard æ—¥å¿—")
    
    with open(results_path, "a", newline="", encoding="utf-8") as f:
        fieldnames = [
            "use_positional_encoding",
            "L_embed",
            "hidden_dim",
            "num_layers",
            "epochs",
            "learning_rate",
            "batch_size",
            "image_size",
            "final_psnr",
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not results_exists:
            writer.writeheader()

        for run_idx, (use_pe, l_embed, hidden_dim, num_layers) in enumerate(
            param_combos, start=1
        ):
            config = {
                "mode": cfg["mode"],
                "L_embed": l_embed,
                "hidden_dim": hidden_dim,
                "output_dim": output_dim,
                "num_layers": num_layers,
                "use_positional_encoding": use_pe,
            }

            run_name = f"pe{int(bool(use_pe))}_L{l_embed}_H{hidden_dim}_N{num_layers}"
            run_dir = os.path.join(log_dir, run_name)
            os.makedirs(run_dir, exist_ok=True)

            # åˆå§‹åŒ– TensorBoard logger
            tb_dir = os.path.join(tb_base_dir, run_name)
            tb_logger = TensorBoardLogger(tb_dir)

            save_intermediate = isinstance(save_every, int) and save_every > 0
            if save_intermediate:
                steps_dir = os.path.join(run_dir, "steps")
                os.makedirs(steps_dir, exist_ok=True)

            print(f">>> [{run_idx}/{len(param_combos)}] é…ç½®: {run_name}, Steps={epochs}")

            model = NeuralField(config).to(device)
            optimizer = optim.Adam(model.parameters(), lr=learning_rate)

            # è®­ç»ƒå¾ªç¯
            for i in tqdm(range(epochs)):
                if batch_size is None:
                    # å…¨å›¾è®­ç»ƒ
                    pred_rgb = model(coords)
                    loss = loss_fn(pred_rgb, gt_rgb)
                else:
                    # éšæœºæ‰¹é‡é‡‡æ ·
                    indices = torch.randint(0, total_pixels, (batch_size,), device=device)
                    batch_coords = coords[indices]
                    batch_gt_rgb = gt_rgb[indices]
                    pred_rgb = model(batch_coords)
                    loss = loss_fn(pred_rgb, batch_gt_rgb)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # è®°å½•åˆ° TensorBoard
                if (i + 1) % log_every == 0:
                    psnr = compute_psnr(loss.item())
                    tb_logger.log_scalar('Train/Loss', loss.item(), i + 1)
                    tb_logger.log_scalar('Train/PSNR', psnr, i + 1)

                # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
                if save_intermediate and (i + 1) % save_every == 0:
                    with torch.no_grad():
                        intermediate_img = model(coords).cpu().numpy().reshape(h, w, 3)
                    plt.imsave(
                        os.path.join(steps_dir, f"step_{i+1:05d}.png"),
                        intermediate_img,
                    )

            # è®­ç»ƒå®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆç»“æœ
            with torch.no_grad():
                final_pred = model(coords)
                final_img = final_pred.cpu().numpy().reshape(h, w, 3)
                final_loss = loss_fn(final_pred, gt_rgb).item()

            final_img_path = os.path.join(run_dir, "final.png")
            plt.imsave(final_img_path, final_img)
            model_path = os.path.join(run_dir, "model_final.pth")
            torch.save(
                {"model_state_dict": model.state_dict(), "config": config},
                model_path,
            )

            final_psnr = compute_psnr(final_loss)
            writer.writerow(
                {
                    "use_positional_encoding": use_pe,
                    "L_embed": l_embed,
                    "hidden_dim": hidden_dim,
                    "num_layers": num_layers,
                    "epochs": epochs,
                    "learning_rate": learning_rate,
                    "batch_size": batch_size,
                    "image_size": image_size,
                    "final_psnr": final_psnr,
                }
            )
            f.flush()
            
            # è®°å½•æœ€ç»ˆ PSNR åˆ° TensorBoard
            tb_logger.log_scalar('Final/PSNR', final_psnr, epochs)
            tb_logger.close()

            print(f">>> Done! Final PSNR: {final_psnr:.2f} dB")


def run_part2(cfg, args):
    """Part 2: NeRF 3Dåœºæ™¯é‡å»ºï¼Œè®­ç»ƒå’Œè¯„ä¼°"""
    if not args.data_dir:
        raise ValueError("Part 2 requires --data_dir pointing to a NeRF dataset root.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")

    # è¯»å–æ¸²æŸ“å’Œè®­ç»ƒé…ç½®
    downscale = cfg.get("downscale", 1)
    white_bkgd = cfg.get("white_bkgd", True)
    scene_scale = cfg.get("scene_scale", 1.0)
    near = float(cfg.get("near", 2.0))  # è¿‘å¹³é¢
    far = float(cfg.get("far", 6.0))  # è¿œå¹³é¢
    n_samples = cfg.get("n_samples", 64)  # è®­ç»ƒé‡‡æ ·ç‚¹æ•°
    render_n_samples = cfg.get("render_n_samples", n_samples)  # æ¸²æŸ“é‡‡æ ·ç‚¹æ•°
    batch_size = cfg.get("batch_size", 4096)  # æ¯æ‰¹å…‰çº¿æ•°
    train_iters = cfg.get("train_iters", 20000)  # è®­ç»ƒè¿­ä»£æ•°
    learning_rate = cfg.get("learning_rate", 5e-4)
    log_every = cfg.get("log_every", 100)
    save_every = cfg.get("save_every", 2000)
    chunk = cfg.get("chunk", 8192)  # æ¸²æŸ“å—å¤§å°
    log_dir = cfg.get("log_dir", "output/part2")
    if args.render_chunk:
        chunk = args.render_chunk

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    ckpt_dir = os.path.join(log_dir, "checkpoints")
    render_dir = os.path.join(log_dir, "renders")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(render_dir, exist_ok=True)

    # åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
    train_set = BlenderDataset(
        root_dir=args.data_dir,
        split="train",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    test_split = "test"
    test_meta = os.path.join(args.data_dir, "transforms_test.json")
    if not os.path.exists(test_meta):
        test_split = "val"
    test_set = BlenderDataset(
        root_dir=args.data_dir,
        split=test_split,
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )

    # åˆå§‹åŒ–æ¨¡å‹
    model = NeuralField(cfg).to(device)
    if args.checkpoint:
        ckpt = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        print(f">>> Loaded checkpoint: {args.checkpoint}")

    # è®­ç»ƒé˜¶æ®µ
    if not args.eval_only:
        # åˆå§‹åŒ– TensorBoard
        tb_dir = os.path.join(log_dir, "tensorboard")
        tb_logger = TensorBoardLogger(tb_dir)
        print(f">>> tensorboard --logdir={tb_dir} æŸ¥çœ‹ TensorBoard æ—¥å¿—")
        
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        loss_fn = nn.MSELoss()

        print(">>> Start Training Part 2 (NeRF)...")
        model.train()
        for step in range(1, train_iters + 1):
            # éšæœºé‡‡æ ·å…‰çº¿å¹¶æ¸²æŸ“
            rays_o, rays_d, target_rgba = train_set.sample_random_rays(batch_size, device)
            
            # åˆ†ç¦»å¹¶åˆæˆ target (ä½¿ç”¨å›ºå®šèƒŒæ™¯)
            target_rgb = target_rgba[:, :3]
            target_alpha = target_rgba[:, 3:4]
            if white_bkgd:
                target = target_rgb * target_alpha + (1.0 - target_alpha)
            else:
                target = target_rgb * target_alpha
            
            pred_rgb, _, _ = render_rays(
                model=model,
                rays_o=rays_o,
                rays_d=rays_d,
                near=near,
                far=far,
                n_samples=n_samples,
                perturb=True,
                white_bkgd=white_bkgd,
            )
            loss = loss_fn(pred_rgb, target)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if step % log_every == 0:
                psnr = compute_psnr(loss.item())
                print(
                    f">>> Step {step}/{train_iters} | Loss {loss.item():.6f} | PSNR {psnr:.2f} dB"
                )
                
                # è®°å½•åˆ° TensorBoard
                tb_logger.log_scalar('Train/Loss', loss.item(), step)
                tb_logger.log_scalar('Train/PSNR', psnr, step)

            if save_every and step % save_every == 0:
                ckpt_path = os.path.join(ckpt_dir, f"model_step_{step:06d}.pth")
                torch.save(
                    {"model_state_dict": model.state_dict(), "config": cfg}, ckpt_path
                )

        final_path = os.path.join(ckpt_dir, "model_final.pth")
        torch.save({"model_state_dict": model.state_dict(), "config": cfg}, final_path)
        
        tb_logger.close()
        print(f">>> è®­ç»ƒå®Œæˆï¼TensorBoard æ—¥å¿—å·²ä¿å­˜åˆ°: {tb_dir}")

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # è¯„ä¼°é˜¶æ®µï¼šæ¸²æŸ“æµ‹è¯•é›†
    model.eval()
    print(f">>> Rendering {test_split} set...")
    psnrs = []
    with torch.no_grad():
        for idx in range(len(test_set)):
            rays_o, rays_d, target = test_set.get_image_rays(idx, device)
            pred = render_image_safe(
                render_fn=render_image,
                model=model,
                rays_o=rays_o,
                rays_d=rays_d,
                near=near,
                far=far,
                n_samples=render_n_samples,
                chunk=chunk,
                white_bkgd=white_bkgd,
            )
            pred = torch.clamp(pred, 0.0, 1.0)
            psnr = compute_psnr_torch(pred, target)
            psnrs.append(psnr)
            plt.imsave(
                os.path.join(render_dir, f"test_{idx:03d}.png"),
                pred.cpu().numpy(),
            )

    avg_psnr = float(np.mean(psnrs)) if psnrs else 0.0
    print(f">>> Test PSNR: {avg_psnr:.2f} dB")
    print(f">>> Rendered images saved to: {render_dir}")


def run_part2_instant(cfg, args):
    """Part 2 Instant: Instant-NeRF åŠ é€Ÿè®­ç»ƒ"""
    if not args.data_dir:
        raise ValueError("Part 2 Instant requires --data_dir pointing to a NeRF dataset root.")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type != 'cuda':
        print("!!! Instant-NeRF åœ¨ CPU ä¸Šæ€§èƒ½æ— æ³•å‘æŒ¥ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨ CUDA GPU")

    # è¯»å–æ¸²æŸ“å’Œè®­ç»ƒé…ç½®
    downscale = cfg.get("downscale", 2)
    white_bkgd = cfg.get("white_bkgd", True)
    scene_scale = cfg.get("scene_scale", 1.0)
    near = float(cfg.get("near", 2.0))
    far = float(cfg.get("far", 6.0))
    n_samples = cfg.get("n_samples", 32)  # Instant-NeRF éœ€è¦æ›´å°‘é‡‡æ ·ç‚¹
    render_n_samples = cfg.get("render_n_samples", n_samples)
    batch_size = cfg.get("batch_size", 8192)  # Instant-NeRF ä½¿ç”¨æ›´å¤§æ‰¹é‡
    train_iters = cfg.get("train_iters", 5000)  # Instant-NeRF è®­ç»ƒæ›´å¿«
    learning_rate = cfg.get("learning_rate", 0.01)  # Instant-NeRF ä½¿ç”¨é«˜å­¦ä¹ ç‡
    log_every = cfg.get("log_every", 50)
    save_every = cfg.get("save_every", 500)
    chunk = cfg.get("chunk", 16384)  # æ›´å¤§çš„æ¸²æŸ“å—
    log_dir = cfg.get("log_dir", "output/part2_instant")
    
    # è·å–æ•°æ®é›†åç§°å¹¶æ·»åŠ åˆ°è¾“å‡ºè·¯å¾„
    dataset_name = os.path.basename(args.data_dir)
    log_dir = os.path.join(log_dir, dataset_name)
    
    if args.render_chunk:
        chunk = args.render_chunk

    # Instant-NeRF ç‰¹æœ‰é…ç½®
    use_density_grid = cfg.get("use_density_grid", True)
    grid_resolution = cfg.get("grid_resolution", 128)
    grid_threshold = cfg.get("grid_threshold", 0.01)
    grid_update_interval = cfg.get("grid_update_interval", 16)
    grid_warmup_iters = cfg.get("grid_warmup_iters", 256)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(log_dir, exist_ok=True)
    ckpt_dir = os.path.join(log_dir)
    render_dir = os.path.join(log_dir, "renders")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(render_dir, exist_ok=True)

    # åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
    train_set = BlenderDataset(
        root_dir=args.data_dir,
        split="train",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    # åŠ è½½æµ‹è¯•é›†
    test_split = "test"
    test_meta = os.path.join(args.data_dir, "transforms_test.json")
    if not os.path.exists(test_meta):
        test_split = "val"
    test_set = BlenderDataset(
        root_dir=args.data_dir,
        split=test_split,
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )

    
    # åªåœ¨è®­ç»ƒæ¨¡å¼ä¸‹åˆ’åˆ†éªŒè¯é›†
    if not args.eval_only:
        # ä»æµ‹è¯•é›†ä¸­éšæœºæŠ½å–30%ä½œä¸ºéªŒè¯é›†
        import random
        n_test = len(test_set.images)
        n_val = int(n_test * 0.3)
        val_indices = random.sample(range(n_test), n_val)
        test_indices = [i for i in range(n_test) if i not in val_indices]
        
        # åˆ›å»ºéªŒè¯é›†
        val_set = BlenderDataset(
            root_dir=args.data_dir,
            split=test_split,
            downscale=downscale,
            white_bkgd=white_bkgd,
            scene_scale=scene_scale,
        )
        val_set.images = test_set.images[val_indices]
        val_set.poses = test_set.poses[val_indices]
        
        # ä¸ç¼©å‡æµ‹è¯•é›†ï¼Œä¿æŒå®Œæ•´
        print(f">>> æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {len(train_set.images)} å¼  | éªŒè¯é›† {len(val_set.images)} å¼  | æµ‹è¯•é›† {len(test_set.images)} å¼ ")
    else:
        # è¯„ä¼°æ¨¡å¼ï¼šä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†
        print(f">>> è¯„ä¼°ä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›† {len(test_set.images)} å¼ ")
        val_set = None

    # åˆå§‹åŒ–æ¨¡å‹
    print(">>> åˆå§‹åŒ– Instant-NeRF æ¨¡å‹...")
    model = NeuralField(cfg).to(device)
    

    # è‡ªåŠ¨æ£€æµ‹ scene_boundï¼ˆå¦‚æœé…ç½®ä¸­è®¾ç½®ä¸º "auto"ï¼‰
    if cfg.get('scene_bound') == 'auto':
        # ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†å§¿æ€ä¸­æå–ç›¸æœºä½ç½®
        all_poses = torch.cat([train_set.poses, test_set.poses], dim=0)
        cam_positions = all_poses[:, :3, 3].cpu().numpy()
        
        # è®¡ç®—ç›¸æœºåˆ°åŸç‚¹çš„æœ€å¤§è·ç¦»
        max_distance = np.max(np.linalg.norm(cam_positions, axis=1))
        
        # æ·»åŠ 5%ä½™é‡ä½œä¸º scene_bound
        scene_bound_auto = max_distance * 1.05
        cfg['scene_bound'] = scene_bound_auto
        print(f">>> è‡ªåŠ¨æ£€æµ‹ scene_bound: {scene_bound_auto:.2f}ï¼ˆåŸºäºç›¸æœºæœ€å¤§è·ç¦» {max_distance:.2f}ï¼‰")


    # åˆå§‹åŒ–å æ®ç½‘æ ¼
    density_grid = None
    active_ratio = 1.0  # åˆå§‹åŒ–æ´»è·ƒæ¯”ä¾‹ï¼ˆwarmup æœŸé—´é»˜è®¤ 100%ï¼‰
    if use_density_grid:
        from src.renderer import DensityGrid
        density_grid = DensityGrid(
            resolution=grid_resolution,
            bound=cfg.get('scene_bound', 1.5),
            threshold=grid_threshold
        ).to(device)
        print(f">>> Density Grid å·²å¯ç”¨: {grid_resolution}Â³ åˆ†è¾¨ç‡")
    else:
        print(">>> Density Grid å·²ç¦ç”¨ï¼ˆæ€§èƒ½ä¼šé™ä½ï¼‰")
    
    # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆåŒ…æ‹¬ density_gridï¼‰
    if args.checkpoint:
        ckpt = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        if density_grid is not None and "density_grid" in ckpt:
            density_grid.load_state_dict(ckpt["density_grid"])
            print(f">>> Loaded checkpoint with DensityGrid: {args.checkpoint} (Step {ckpt.get('step', 'æœªçŸ¥')} | Val PSNR {ckpt.get('val_psnr', None):.2f} dB)")
        else:
            print(f">>> Loaded checkpoint: {args.checkpoint} (Step {ckpt.get('step', 'æœªçŸ¥')} | Val PSNR {ckpt.get('val_psnr', None):.2f} dB)")

    # è®­ç»ƒé˜¶æ®µ
    if not args.eval_only:
        # åˆå§‹åŒ– TensorBoard
        tb_dir = os.path.join(log_dir, "tensorboard", get_exp_name(cfg))
        tb_logger = TensorBoardLogger(tb_dir)
        
        # AdamW ä¼˜åŒ–å™¨
        weight_decay = cfg.get('weight_decay', 1e-5)
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        
        # Cosine è¡°å‡è°ƒåº¦å™¨
        eta_min = cfg.get('eta_min', 1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_iters, eta_min=eta_min)
        
        # éšæœºèƒŒæ™¯å¢å¼º
        use_random_bg = cfg.get('use_random_bg', False)
        
        # TV Loss (Total Variation) - æƒ©ç½š HashGrid ç›¸é‚»ç‰¹å¾å·®å¼‚ï¼Œæ¶ˆé™¤è¾¹ç¼˜æ¯›åˆº
        use_tv_loss = cfg.get('use_tv_loss', True)
        tv_loss_weight = float(cfg.get('tv_loss_weight', 1e-6))
        
        loss_fn = nn.MSELoss()

        print(f">>> ç›®æ ‡: {train_iters} æ­¥")
        print(f">>> å­¦ä¹ ç‡: {learning_rate} (Cosine è¡°å‡è‡³ {eta_min})")
        print(f">>> æ‰¹é‡å¤§å°: {batch_size}")
        print(f">>> é‡‡æ ·ç‚¹æ•°: {n_samples} ")
        if use_tv_loss:
            print(f">>> æ­£åˆ™åŒ–: TV Loss å·²å¯ç”¨ (weight={tv_loss_weight:.0e})")
        if use_random_bg:
            random_bg_start = cfg.get('random_bg_start', 0)
            if random_bg_start > 0:
                print(f">>> æ•°æ®å¢å¼º: éšæœºèƒŒæ™¯å¢å¼º ({random_bg_start} æ­¥åå¼€å¯)")
            else:
                print(f">>> æ•°æ®å¢å¼º: éšæœºèƒŒæ™¯å¢å¼º (å…¨ç¨‹å¯ç”¨)")
        print(f">>> tensorboard --logdir={os.path.join(log_dir, 'tensorboard')} æŸ¥çœ‹ TensorBoard æ—¥å¿—")
        
        # åˆå§‹åŒ–æœ€ä½³éªŒè¯é›†PSNRè·Ÿè¸ª
        best_val_psnr = 0.0
        
        model.train()
        for step in range(1, train_iters + 1):
            # éšæœºé‡‡æ ·å…‰çº¿å¹¶æ¸²æŸ“ (è¿”å› RGBA 4é€šé“)
            rays_o, rays_d, target_rgba = train_set.sample_random_rays(batch_size, device)
            
            # åˆ†ç¦» RGB å’Œ Alpha é€šé“
            target_rgb = target_rgba[:, :3]    # [B, 3]
            target_alpha = target_rgba[:, 3:4] # [B, 1]
            
            # éšæœºèƒŒæ™¯å¢å¼ºï¼šä» random_bg_start æ­¥å¼€å§‹å¯ç”¨
            if use_random_bg and step >= random_bg_start:
                bg_color = torch.rand(3, device=device)
            else:
                bg_color = torch.ones(3, device=device) if white_bkgd else torch.zeros(3, device=device)
            
            # åŠ¨æ€åˆæˆ target: RGB * Alpha + bg_color * (1 - Alpha)
            target = target_rgb * target_alpha + bg_color * (1.0 - target_alpha)
            
            # ä½¿ç”¨å æ®ç½‘æ ¼åŠ é€Ÿæ¸²æŸ“
            pred_rgb, _, _ = render_rays(
                model=model,
                rays_o=rays_o,
                rays_d=rays_d,
                near=near,
                far=far,
                n_samples=n_samples,
                perturb=True,
                white_bkgd=white_bkgd,
                density_grid=density_grid,
                bg_color=bg_color,
            )
            loss_rgb = loss_fn(pred_rgb, target)
            
            # TV Loss - æƒ©ç½š HashGrid å“ˆå¸Œè¡¨ä¸­ç›¸é‚»æ¡ç›®çš„ç‰¹å¾å·®å¼‚
            loss_tv = torch.tensor(0.0, device=device)
            if use_tv_loss and hasattr(model, 'representation') and hasattr(model.representation, 'encoding'):
                hash_params = model.representation.encoding.params  # [N_entries, n_features]
                tv_diff = torch.abs(hash_params[1:] - hash_params[:-1])  # L1 èŒƒæ•°
                loss_tv = torch.mean(tv_diff) * tv_loss_weight
            
            loss = loss_rgb + loss_tv

            optimizer.zero_grad()
            loss.backward()
            
            # åˆ†åˆ«è£å‰ªæ•£åˆ—è¡¨å’Œ MLP çš„æ¢¯åº¦
            if hasattr(model, 'representation'):
                torch.nn.utils.clip_grad_norm_(model.representation.parameters(), max_norm=1.0)
            if hasattr(model, 'decoder'):
                torch.nn.utils.clip_grad_norm_(model.decoder.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()  # Cosine è¡°å‡

            # åŠ¨æ€ç½‘æ ¼æ›´æ–°ï¼šå‰ 10% æ­¥æ•°æ¯ 16 æ­¥æ›´æ–°ï¼Œ10%-50% æ¯ 64 æ­¥ï¼Œ50% åæ¯ 256 æ­¥
            # è®­ç»ƒåæœŸï¼ˆgrid_stop_ratio åï¼‰åœæ­¢æ›´æ–°
            grid_stop_ratio = cfg.get('grid_stop_ratio', 0.9)
            if step < train_iters * grid_stop_ratio:
                if step < train_iters * 0.1:
                    dynamic_interval = 32
                elif step < train_iters * 0.5:
                    dynamic_interval = 128
                else:
                    dynamic_interval = 512
                
                if density_grid is not None and density_grid.should_update(step, dynamic_interval, grid_warmup_iters):
                    model.eval()
                    active_ratio = density_grid.update(model, device=device, time=None)
                    model.train()

            # æ—¥å¿—è¾“å‡ºå’Œ TensorBoard è®°å½•
            if step % log_every == 0:
                psnr = compute_psnr(loss_rgb.item())
                skip_info = ""
                if density_grid is not None:
                    skip_info = f" | Skip: {(1-active_ratio)*100:.1f}%"
                print(
                    f">>> Step {step}/{train_iters} | Loss {loss.item():.6f} | PSNR {psnr:.2f} dB{skip_info}"
                )
                
                # è®°å½•åˆ° TensorBoard
                tb_logger.log_scalar('Train/Loss', loss_rgb.item(), step)
                tb_logger.log_scalar('Train/PSNR', psnr, step)
                if use_tv_loss:
                    tb_logger.log_scalar('Train/TV_Loss', loss_tv.item(), step)
                if density_grid is not None:
                    tb_logger.log_scalar('Train/ActiveRatio', active_ratio, step)
            
            # å®šæœŸéªŒè¯é›†è¯„ä¼°
            val_every = cfg.get("val_every", 500)
            if step % val_every == 0:
                model.eval()
                val_psnrs = []
                with torch.no_grad():
                    for idx in range(len(val_set.images)):
                        rays_o, rays_d, target = val_set.get_image_rays(idx, device)
                        rays_o = rays_o.reshape(-1, 3)
                        rays_d = rays_d.reshape(-1, 3)
                        target = target.reshape(-1, 3)
                        
                        # åˆ†å—æ¸²æŸ“éªŒè¯é›†
                        pred_chunks = []
                        for i in range(0, rays_o.shape[0], chunk):
                            pred_chunk, _, _ = render_rays(
                                model=model,
                                rays_o=rays_o[i:i+chunk],
                                rays_d=rays_d[i:i+chunk],
                                near=near,
                                far=far,
                                n_samples=render_n_samples,
                                perturb=False,
                                white_bkgd=white_bkgd,
                                density_grid=density_grid,
                            )
                            pred_chunks.append(pred_chunk)
                        pred = torch.cat(pred_chunks, dim=0)
                        val_psnr = compute_psnr_torch(pred, target)
                        val_psnrs.append(val_psnr)
                
                avg_val_psnr = float(np.mean(val_psnrs))
                print(f"    [Validation] PSNR: {avg_val_psnr:.2f} dB", end="")
                
                # è®°å½•éªŒè¯é›† PSNR åˆ° TensorBoard
                tb_logger.log_scalar('Validation/PSNR', avg_val_psnr, step)
                
                # åªåœ¨éªŒè¯é›†PSNRæå‡æ—¶ä¿å­˜æ¨¡å‹
                if avg_val_psnr > best_val_psnr:
                    best_val_psnr = avg_val_psnr
                    best_path = os.path.join(ckpt_dir, f"best_model.pth")
                    save_dict = {
                        "model_state_dict": model.state_dict(),
                        "config": cfg,
                        "step": step,
                        "val_psnr": best_val_psnr
                    }
                    if density_grid is not None:
                        save_dict["density_grid"] = density_grid.state_dict()
                    torch.save(save_dict, best_path)
                    print(f" | ğŸŒŸ New Best Model! Saved to {best_path}")
                else:
                    print()
                
                model.train()

        print(f"\n>>> è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›† PSNR: {best_val_psnr:.2f} dB")
        tb_logger.close()

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # è¯„ä¼°é˜¶æ®µ
    if args.eval_only:
        import random
        import shutil
        import subprocess
        
        # åˆ¤æ–­æ˜¯å¦é¡ºåºæ¸²æŸ“å¹¶ç”Ÿæˆè§†é¢‘ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨æµ‹è¯•é›†ï¼‰
        if args.render_n == -1:
            n_render = len(test_set.images)
            render_indices = list(range(n_render))
            
            # åˆ›å»ºä¸´æ—¶å›¾ç‰‡ç›®å½•å’Œè§†é¢‘ç›®å½•
            picture_dir = os.path.join(log_dir, "picture")
            video_dir = os.path.join(log_dir)
            os.makedirs(picture_dir, exist_ok=True)
            os.makedirs(video_dir, exist_ok=True)
            
            print(f"\n>>> æ¸²æŸ“å…¨éƒ¨æµ‹è¯•é›†å›¾ç‰‡ï¼ˆæŒ‰é¡ºåº {n_render} å¼ ï¼‰ç”¨äºç”Ÿæˆè§†é¢‘...")
            
            model.eval()
            psnrs = []
            with torch.no_grad():
                for i, idx in enumerate(tqdm(render_indices)):
                    rays_o, rays_d, target = test_set.get_image_rays(idx, device)
                    H, W = rays_o.shape[:2]
                    rays_o = rays_o.reshape(-1, 3)
                    rays_d = rays_d.reshape(-1, 3)
                    
                    # ä½¿ç”¨ density_grid åŠ é€Ÿæ¸²æŸ“
                    pred_chunks = []
                    for j in range(0, rays_o.shape[0], chunk):
                        pred_chunk, _, _ = render_rays(
                            model=model,
                            rays_o=rays_o[j:j+chunk],
                            rays_d=rays_d[j:j+chunk],
                            near=near,
                            far=far,
                            n_samples=render_n_samples,
                            perturb=False,
                            white_bkgd=white_bkgd,
                            density_grid=density_grid,  # ä½¿ç”¨å æ®ç½‘æ ¼åŠ é€Ÿ
                        )
                        pred_chunks.append(pred_chunk)
                    
                    pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                    pred = torch.clamp(pred, 0.0, 1.0)
                    psnr = compute_psnr_torch(pred, target)
                    psnrs.append(psnr)
                    
                    # ä¿å­˜ä¸ºè¿ç»­ç¼–å·çš„å¸§
                    plt.imsave(
                        os.path.join(picture_dir, f"frame_{i:03d}.png"),
                        pred.cpu().numpy(),
                    )
            
            avg_psnr = float(np.mean(psnrs))
            print(f"\n>>> æ¸²æŸ“å®Œæˆï¼å¹³å‡ PSNR: {avg_psnr:.2f} dB")
            
            # ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘
            dataset_name = os.path.basename(args.data_dir)
            video_path = os.path.join(video_dir, f"{dataset_name}_24fps.mp4")
            print(f"\n>>> ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘...")
            try:
                cmd = [
                    "ffmpeg", "-y",
                    "-framerate", "24",
                    "-i", os.path.join(picture_dir, "frame_%03d.png"),
                    "-c:v", "libx264",
                    "-pix_fmt", "yuv420p",
                    "-crf", "18",
                    video_path
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    print(f">>> è§†é¢‘å·²ä¿å­˜: {video_path}")
                    print(f">>> è§†é¢‘æ—¶é•¿: {n_render/24:.1f}ç§’ ({n_render} å¸§ @ 24fps)")
                    
                    # åˆ é™¤ä¸´æ—¶å›¾ç‰‡ç›®å½•
                    shutil.rmtree(picture_dir)
                    print(f">>> å·²æ¸…ç†ä¸´æ—¶å›¾ç‰‡ç›®å½•")
                else:
                    print(f"!!! ffmpeg æ‰§è¡Œå¤±è´¥:\n{result.stderr}")
            except FileNotFoundError:
                print("!!! æœªæ‰¾åˆ° ffmpegï¼Œè¯·å…ˆå®‰è£…: sudo apt install ffmpeg")
            except Exception as e:
                print(f"!!! è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
        else:
            # éšæœºæ¸²æŸ“æŒ‡å®šæ•°é‡çš„å›¾ç‰‡
            n_render = min(args.render_n, len(test_set.images))
            render_indices = random.sample(range(len(test_set.images)), n_render)
            
            print(f"\n>>> æ¸²æŸ“æµ‹è¯•é›†å›¾ç‰‡ï¼ˆéšæœº {n_render} å¼ ï¼‰...")
            os.makedirs(render_dir, exist_ok=True)
            psnrs = []
            
            model.eval()
            with torch.no_grad():
                for i, idx in enumerate(tqdm(render_indices)):
                    rays_o, rays_d, target = test_set.get_image_rays(idx, device)
                    H, W = rays_o.shape[:2]
                    rays_o = rays_o.reshape(-1, 3)
                    rays_d = rays_d.reshape(-1, 3)
                    
                    # ä½¿ç”¨ density_grid åŠ é€Ÿæ¸²æŸ“
                    pred_chunks = []
                    for j in range(0, rays_o.shape[0], chunk):
                        pred_chunk, _, _ = render_rays(
                            model=model,
                            rays_o=rays_o[j:j+chunk],
                            rays_d=rays_d[j:j+chunk],
                            near=near,
                            far=far,
                            n_samples=render_n_samples,
                            perturb=False,
                            white_bkgd=white_bkgd,
                            density_grid=density_grid,  
                        )
                        pred_chunks.append(pred_chunk)
                    
                    pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                    pred = torch.clamp(pred, 0.0, 1.0)
                    psnr = compute_psnr_torch(pred, target)
                    psnrs.append(psnr)
                    
                    # ä¿å­˜æ¸²æŸ“å›¾ç‰‡ï¼ˆå¸¦PSNRä¿¡æ¯ï¼‰
                    plt.imsave(
                        os.path.join(render_dir, f"render_{idx:03d}_psnr{psnr:.2f}.png"),
                        pred.cpu().numpy(),
                    )
            
            avg_psnr = float(np.mean(psnrs))
            print(f"\n>>> æ¸²æŸ“å®Œæˆï¼å¹³å‡ PSNR: {avg_psnr:.2f} dB")
            print(f">>> ä¿å­˜è·¯å¾„: {render_dir}")
        return
    
    # è®­ç»ƒåçš„æ ‡å‡†è¯„ä¼°ï¼šè®¡ç®—æµ‹è¯•é›†PSNR
    model.eval()
    print(f"\n>>> è¯„ä¼° {test_split} é›†...")
    psnrs = []
    with torch.no_grad():
        for idx in tqdm(range(len(test_set))):
            rays_o, rays_d, target = test_set.get_image_rays(idx, device)
            H, W = rays_o.shape[:2]
            rays_o = rays_o.reshape(-1, 3)
            rays_d = rays_d.reshape(-1, 3)
            
            # ä½¿ç”¨ density_grid åŠ é€Ÿæ¸²æŸ“
            pred_chunks = []
            for j in range(0, rays_o.shape[0], chunk):
                pred_chunk, _, _ = render_rays(
                    model=model,
                    rays_o=rays_o[j:j+chunk],
                    rays_d=rays_d[j:j+chunk],
                    near=near,
                    far=far,
                    n_samples=render_n_samples,
                    perturb=False,
                    white_bkgd=white_bkgd,
                    density_grid=density_grid,
                )
                pred_chunks.append(pred_chunk)
            
            pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
            pred = torch.clamp(pred, 0.0, 1.0)
            psnr = compute_psnr_torch(pred, target)
            psnrs.append(psnr)

    avg_psnr = float(np.mean(psnrs)) if psnrs else 0.0
    print(f"\n{'='*60}")
    print(f">>> Instant-NeRF è¯„ä¼°ç»“æœ")
    print(f">>> æµ‹è¯•é›†å¹³å‡ PSNR: {avg_psnr:.2f} dB")
    print(f">>> æœ€ä½³éªŒè¯é›† PSNR: {best_val_psnr:.2f} dB" if not args.eval_only else "")
    print(f"{'='*60}")


def run_part3(cfg, args):
    """Part 3: åŠ¨æ€ NeRF"""
    if not args.data_dir:
        raise ValueError("Part 3 requires --data_dir pointing to a dynamic NeRF dataset root.")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")

    # è¯»å–æ¸²æŸ“å’Œè®­ç»ƒé…ç½®
    downscale = cfg.get("downscale", 1)
    white_bkgd = cfg.get("white_bkgd", True)
    scene_scale = cfg.get("scene_scale", 1.0)
    near = float(cfg.get("near", 2.0))
    far = float(cfg.get("far", 6.0))
    n_samples = cfg.get("n_samples", 64)
    render_n_samples = cfg.get("render_n_samples", n_samples)
    batch_size = cfg.get("batch_size", 4096)
    train_iters = cfg.get("train_iters", 20000)
    learning_rate = cfg.get("learning_rate", 5e-4)
    log_every = cfg.get("log_every", 100)
    save_every = cfg.get("save_every", 2000)
    chunk = cfg.get("chunk", 8192)
    deformation_reg_weight = cfg.get("deformation_reg_weight", 1e-4) # å˜å½¢æ­£åˆ™åŒ–æƒé‡
    render_n = args.render_n
    if args.render_chunk is not None:
        chunk = args.render_chunk
    log_dir = cfg.get("log_dir", "output/part3")
    
    # è·å–æ•°æ®é›†åç§°å¹¶æ·»åŠ åˆ°è¾“å‡ºè·¯å¾„
    dataset_name = os.path.basename(args.data_dir)
    log_dir = os.path.join(log_dir, dataset_name)

    os.makedirs(log_dir, exist_ok=True)
    ckpt_dir = os.path.join(log_dir)
    render_dir = os.path.join(log_dir, "renders")
    val_render_dir = os.path.join(log_dir, "val_renders")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(render_dir, exist_ok=True)
    os.makedirs(val_render_dir, exist_ok=True)

    from src.dataset import DynamicDataset
    
    train_set = DynamicDataset(
        root_dir=args.data_dir,
        split="train",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    # åŠ è½½éªŒè¯é›†
    val_set = DynamicDataset(
        root_dir=args.data_dir,
        split="val",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    # åŠ è½½æµ‹è¯•é›†
    test_split = "test"
    test_meta = os.path.join(args.data_dir, "transforms_test.json")
    if not os.path.exists(test_meta):
        test_split = "val"
    test_set = DynamicDataset(
        root_dir=args.data_dir,
        split=test_split,
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    if not args.eval_only:
        print(f">>> æ•°æ®é›†: è®­ç»ƒé›† {len(train_set.images)} å¼  | éªŒè¯é›† {len(val_set.images)} å¼  | æµ‹è¯•é›† {len(test_set.images)} å¼ ")
    else:
        print(f">>> æ•°æ®é›†: æµ‹è¯•é›† {len(test_set.images)} å¼ ")

    # æ¨¡å‹åˆå§‹åŒ–
    from src.core import NeuralField
    model = NeuralField(cfg).to(device)
    
    # å¦‚æœä½¿ç”¨ instant æ¨¡å¼ï¼Œå¯ç”¨ density_grid
    canonical_type = cfg.get('canonical_type', 'nerf')
    density_grid = None
    active_ratio = 1.0
    if canonical_type == 'instant':
        use_density_grid = cfg.get('use_density_grid', True)
        if use_density_grid:
            from src.renderer import DensityGrid
            grid_resolution = cfg.get('grid_resolution', 128)
            grid_threshold = cfg.get('grid_threshold', 0.01)
            scene_bound = cfg.get('scene_bound', 1.5)
            density_grid = DensityGrid(
                resolution=grid_resolution,
                bound=scene_bound,
                threshold=grid_threshold
            ).to(device)
            print(f">>> Density Grid å·²å¯ç”¨: {grid_resolution}Â³ åˆ†è¾¨ç‡ (Instant-NGP æ¨¡å¼)")
    
    if args.checkpoint:
        ckpt = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        if density_grid is not None and "density_grid" in ckpt:
            density_grid.load_state_dict(ckpt["density_grid"])
        print(f">>> Loaded checkpoint: {args.checkpoint}")

    # è®­ç»ƒé˜¶æ®µ
    if not args.eval_only:
        # åˆå§‹åŒ– TensorBoard
        tb_dir = os.path.join(log_dir, "tensorboard", get_exp_name(cfg))
        tb_logger = TensorBoardLogger(tb_dir)
        
        weight_decay = cfg.get('weight_decay', 1e-5)
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        
        # TiNeuVox æ”¹è¿›ï¼šä½¿ç”¨ CosineAnnealingLR è°ƒåº¦å™¨ï¼Œé˜²æ­¢è®­ç»ƒåæœŸ PSNR éœ‡è¡
        # ä»åˆå§‹å­¦ä¹ ç‡å¹³æ»‘é™è‡³ eta_min
        eta_min = cfg.get('eta_min', 1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_iters, eta_min=eta_min)
        
        use_amp = cfg.get('use_amp', True)
        scaler = torch.amp.GradScaler('cuda', enabled=use_amp)
        
        loss_fn = nn.MSELoss()
        print(">>> å¼€å§‹è®­ç»ƒ Part 3 (Dynamic NeRF)...")
        print(f">>> tensorboard --logdir={os.path.join(log_dir, 'tensorboard')} æŸ¥çœ‹ TensorBoard æ—¥å¿—")
        
        # ======== æ­£åˆ™åŒ–å’Œæ•°æ®å¢å¼ºé…ç½® ========
        
        # A. TV Loss (Total Variation) - æƒ©ç½š HashGrid ç›¸é‚»ç‰¹å¾ä¹‹é—´çš„å·®å¼‚ï¼Œæ¶ˆé™¤è¾¹ç¼˜æ¯›åˆºå’Œæµ®ç‚¹å™ªå£°
        use_tv_loss = cfg.get('use_tv_loss', True) and canonical_type == 'instant'
        tv_loss_weight = cfg.get('tv_loss_weight', 1e-6)
        
        # B. æ—¶é—´å¹³æ»‘æ­£åˆ™åŒ– - ç¡®ä¿è¿åŠ¨åœ¨æ—¶é—´è½´ä¸ŠäºŒé˜¶å¯¼æ•°å¾®å°
        use_temporal_smooth = cfg.get('use_temporal_smooth', True)
        temporal_smooth_weight = cfg.get('temporal_smooth_weight', 1e-4)
        temporal_epsilon = cfg.get('temporal_epsilon', 0.02)  # æ—¶é—´å·® Îµ
        temporal_n_samples = cfg.get('temporal_n_samples', 256)  # é‡‡æ ·ç‚¹æ•°
        
        # C. éšæœºèƒŒæ™¯å¢å¼ºï¼Œæ¯ batch éšæœºä¸€ä¸ªé¢œè‰²
        use_random_bg = cfg.get('use_random_bg', False)
        random_bg_start = cfg.get('random_bg_start', 0) if use_random_bg else float('inf')
        
        # D. æ— ç›‘ç£ä¸€è‡´æ€§çº¦æŸï¼ˆä½“ç§¯å®ˆæ’ï¼‰
        use_unsup_consistency = cfg.get('use_unsupervised_consistency', False)
        unsup_consistency_weight = cfg.get('unsup_consistency_weight', 0.001)
        unsup_n_samples = cfg.get('unsup_n_samples', 512)
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        if use_tv_loss:
            print(f">>> æ­£åˆ™åŒ–: TV Loss å·²å¯ç”¨ (weight={tv_loss_weight:.0e}, æ¶ˆé™¤ç©ºé—´å™ªå£°)")
        if use_temporal_smooth:
            print(f">>> æ­£åˆ™åŒ–: æ—¶é—´å¹³æ»‘å·²å¯ç”¨ (weight={temporal_smooth_weight:.0e}, Îµ={temporal_epsilon}, æ¶ˆé™¤æ—¶é—´æŠ–åŠ¨)")
        if use_random_bg:
            if random_bg_start > 0:
                print(f">>> æ•°æ®å¢å¼º: éšæœºèƒŒæ™¯å¢å¼º ({random_bg_start} æ­¥åå¼€å¯)")
            else:
                print(f">>> æ•°æ®å¢å¼º: éšæœºèƒŒæ™¯å¢å¼º (å…¨ç¨‹å¯åŠ¨)")
        if cfg.get('use_coord_noise', False):
            print(f">>> æ•°æ®å¢å¼º: åæ ‡å™ªå£°å·²å¯ç”¨ (coord_std={cfg.get('coord_noise_std', 0.005)}, time_std={cfg.get('time_noise_std', 0.02)})")
        if use_unsup_consistency:
            print(f">>> æ•°æ®å¢å¼º: æ— ç›‘ç£ä¸€è‡´æ€§çº¦æŸå·²å¯ç”¨ (weight={unsup_consistency_weight}, n_samples={unsup_n_samples})")
        
        # åˆå§‹åŒ–æœ€ä½³éªŒè¯é›†PSNRè·Ÿè¸ª
        best_val_psnr = 0.0

        model.train()
        grid_update_interval = cfg.get('grid_update_interval', 16)
        grid_warmup_iters = cfg.get('grid_warmup_iters', 256)
        
        for step in range(1, train_iters + 1):
            # é‡‡æ ·è¿”å›: rays_o, rays_d, target_rgba [B,4], times
            rays_o, rays_d, target_rgba, times = train_set.sample_random_rays(batch_size, device)
            
            # åˆ†ç¦» RGB å’Œ Alpha é€šé“
            target_rgb = target_rgba[:, :3]    # [B, 3]
            target_alpha = target_rgba[:, 3:4] # [B, 1]
            
            # ======== B. éšæœºèƒŒæ™¯å¢å¼ºï¼ˆå­¦ç•Œæ ‡å‡†åšæ³•ï¼‰========
            # ä» random_bg_start æ­¥å¼€å§‹å¯ç”¨éšæœºèƒŒæ™¯å¢å¼º
            if use_random_bg and step >= random_bg_start:
                bg_color = torch.rand(3, device=device)  # [3] éšæœº RGB
            else:
                bg_color = torch.ones(3, device=device) if white_bkgd else torch.zeros(3, device=device)
            
            # åˆæˆ target: Target = RGB * Alpha + bg_color * (1 - Alpha)
            target = target_rgb * target_alpha + bg_color * (1.0 - target_alpha)
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.amp.autocast('cuda', enabled=use_amp):
                # è°ƒç”¨ render_raysï¼Œä¼ å…¥ç›¸åŒçš„ bg_color
                pred_rgb, _, _, extras = render_rays(
                    model=model,
                    rays_o=rays_o,
                    rays_d=rays_d,
                    near=near,
                    far=far,
                    n_samples=n_samples,
                    perturb=True,
                    times=times,
                    density_grid=density_grid,
                    bg_color=bg_color,  # ä¼ å…¥éšæœºèƒŒæ™¯è‰²
                )
                
                # A. è¾…åŠ©æŸå¤±å‡½æ•°: RGB Loss + Deformation Regularization
                loss_rgb = loss_fn(pred_rgb, target)
                mean_delta_x = extras['mean_delta_x'] # ä» extras è·å–åŠ æƒå¹³å‡å˜å½¢é‡
                loss_reg = torch.mean(mean_delta_x ** 2) * deformation_reg_weight
                
                # TV Loss (Total Variation) - æƒ©ç½š HashGrid å“ˆå¸Œè¡¨ä¸­ç›¸é‚»æ¡ç›®çš„ç‰¹å¾å·®å¼‚
                loss_tv = torch.tensor(0.0, device=device)
                if use_tv_loss and hasattr(model, 'canonical_repr') and hasattr(model.canonical_repr, 'encoding'):
                    # è·å– HashGrid çš„å¯å­¦ä¹ å‚æ•°
                    hash_params = model.canonical_repr.encoding.params  # [N_entries, n_features]
                    
                    # è®¡ç®—ç›¸é‚»å“ˆå¸Œæ¡ç›®ä¹‹é—´çš„ L1 å·®å¼‚ (TV èŒƒæ•°) å¹¶æƒ©ç½š
                    tv_diff = torch.abs(hash_params[1:] - hash_params[:-1])  # [N-1, n_features]
                    loss_tv = torch.mean(tv_diff) * tv_loss_weight
                
                # æ—¶é—´å¹³æ»‘æ­£åˆ™åŒ– - è¦æ±‚è¿åŠ¨åœ¨æ—¶é—´è½´ä¸Šæ˜¯äºŒé˜¶å¯¼æ•°å¾®å°çš„
                loss_temporal = torch.tensor(0.0, device=device)
                # æ¯ 2 æ­¥è®¡ç®—ä¸€æ¬¡ï¼Œå‡å°‘è®¡ç®—å¼€é”€
                if use_temporal_smooth and step > grid_warmup_iters and step % 2 == 0:
                    n_temp = temporal_n_samples
                    scene_bound = cfg.get('scene_bound', 1.2)
                    
                    # éšæœºé‡‡æ ·ç©ºé—´ç‚¹ï¼ˆåœ¨åœºæ™¯è¾¹ç•Œå†…ï¼‰
                    x_temp = (torch.rand(n_temp, 3, device=device) * 2 - 1) * scene_bound
                    
                    # éšæœºé‡‡æ ·æ—¶é—´ç‚¹ tï¼Œç¡®ä¿ t+Îµ ä»åœ¨ [0, 1] èŒƒå›´å†…
                    t_temp = torch.rand(n_temp, 1, device=device) * (1.0 - temporal_epsilon)
                    t_temp_eps = t_temp + temporal_epsilon
                    
                    # è®¡ç®—åŒä¸€ç‚¹åœ¨ä¸¤ä¸ªç›¸é‚»æ—¶åˆ»çš„ä½ç§»
                    feat_x_temp = model.pos_encoder_for_deform(x_temp)
                    feat_t_temp = model.time_encoder(t_temp)
                    feat_t_temp_eps = model.time_encoder(t_temp_eps)
                    
                    delta_x_t = model.deform_net(feat_x_temp, feat_t_temp)        # D(x, t)
                    delta_x_t_eps = model.deform_net(feat_x_temp, feat_t_temp_eps)  # D(x, t+Îµ)
                    
                    # ä½¿ç”¨ L2 èŒƒæ•°æƒ©ç½šå·®å¼‚
                    loss_temporal = torch.mean((delta_x_t - delta_x_t_eps) ** 2) * temporal_smooth_weight * 2  # *2 è¡¥å¿é‡‡æ ·é¢‘ç‡
                
                # æ— ç›‘ç£ä¸€è‡´æ€§çº¦æŸï¼ˆä½“ç§¯å®ˆæ’ï¼‰
                # å¯¹éšæœºæ—¶åˆ»çš„å˜å½¢åœºæ–½åŠ çº¦æŸï¼Œè¦æ±‚ä½ç§»å‡å€¼è¶‹è¿‘äº 0ã€‚å› ä¸ºå…¨å±€ä½“ç§¯åº”è¯¥ä¿æŒå®ˆæ’ï¼Œç‰©ä½“ä¸åº”è¯¥å‡­ç©ºè†¨èƒ€æˆ–æ”¶ç¼©
                loss_unsup = torch.tensor(0.0, device=device)
                # æ¯ 4 æ­¥è®¡ç®—ä¸€æ¬¡ï¼Œå‡å°‘è®¡ç®—å¼€é”€
                if use_unsup_consistency and step > grid_warmup_iters and step % 4 == 0:
                    n_unsup = min(unsup_n_samples, 512)
                    t_rand = torch.rand(n_unsup, 1, device=device)
                    scene_bound = cfg.get('scene_bound', 1.2)
                    x_rand = (torch.rand(n_unsup, 3, device=device) * 2 - 1) * scene_bound
                    
                    # ä»…è·å–å˜å½¢åœºçš„ä½ç§»ï¼ˆä¸éœ€è¦æ¸²æŸ“ï¼‰
                    feat_t_rand = model.time_encoder(t_rand)
                    feat_x_rand = model.pos_encoder_for_deform(x_rand)
                    delta_x_rand = model.deform_net(feat_x_rand, feat_t_rand)
                    
                    # çº¦æŸï¼šå˜å½¢é‡çš„å…¨å±€å‡å€¼åº”è¶‹è¿‘äº 0ï¼ˆä½“ç§¯å®ˆæ’ï¼‰
                    loss_unsup = torch.mean(torch.abs(delta_x_rand.mean(dim=0))) * unsup_consistency_weight * 4  # *4 è¡¥å¿é‡‡æ ·é¢‘ç‡
                
                total_loss = loss_rgb + loss_reg + loss_tv + loss_temporal + loss_unsup

            optimizer.zero_grad()
            # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(total_loss).backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢ DeformNet å’Œ HashGrid åœ¨åŠ¨æ€åœºæ™¯ä¸­æº¢å‡º
            max_grad_norm = cfg.get('max_grad_norm', 1.0)
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
            
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()  # æ‰§è¡Œå­¦ä¹ ç‡è°ƒåº¦
            
            # åˆ†ç¦»losså€¼ï¼Œé˜²æ­¢ä¿ç•™è®¡ç®—å›¾
            loss_rgb_val = loss_rgb.item()
            loss_reg_val = loss_reg.item()
            loss_tv_val = loss_tv.item() if use_tv_loss else 0.0
            loss_temporal_val = loss_temporal.item() if use_temporal_smooth else 0.0
            loss_unsup_val = loss_unsup.item() if use_unsup_consistency else 0.0
            total_loss_val = total_loss.item()
            
            # åªåˆ é™¤extrasé¿å…ç´¯ç§¯ï¼Œå…¶ä»–å˜é‡è®©Pythonè‡ªåŠ¨ç®¡ç†
            del extras
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šåŠ¨æ€è°ƒæ•´ç½‘æ ¼æ›´æ–°é¢‘ç‡
            # å‰ 10% æ­¥æ•°ï¼šæ¯ 16 æ­¥æ›´æ–°ï¼ˆå¿«é€Ÿå»ºç«‹åŒ…ç»œçº¿ï¼‰
            # 10%-50% æ­¥æ•°ï¼šæ¯ 64 æ­¥æ›´æ–°ï¼ˆä¸­æœŸä¼˜åŒ–ï¼‰
            # 50% æ­¥æ•°åï¼šæ¯ 256 æ­¥æ›´æ–°ï¼ˆåæœŸå¾®è°ƒï¼‰
            if step < train_iters * 0.1:
                dynamic_interval = 16
            elif step < train_iters * 0.5:
                dynamic_interval = 64
            else:
                dynamic_interval = 256
            
            if density_grid is not None and density_grid.should_update(step, dynamic_interval, grid_warmup_iters):
                model.eval()
                # TiNeuVox æ”¹è¿›ï¼šæš´åŠ›æ—¶ç©ºæ›´æ–° - ä¸€æ¬¡æ€§é‡‡æ ·å¤šä¸ªæ—¶é—´ç‚¹ï¼Œè®©ç½‘æ ¼å½¢æˆå®Œæ•´çš„"è¿åŠ¨åŒ…ç»œçº¿"
                time_min = train_set.times.min().item()
                time_max = train_set.times.max().item()
                # æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´é‡‡æ ·å¯†åº¦
                n_time_samples = 16 if step < 1000 else 8
                update_times = torch.linspace(time_min, time_max, steps=n_time_samples, device=device)
                
                for i, t_val in enumerate(update_times):
                    # å®Œå…¨ç¦ç”¨è¡°å‡ï¼šä¸¥æ ¼æ—¶ç©ºå¹¶é›†ï¼Œæ°¸ä¹…ä¿ç•™æ‰€æœ‰æ—¶åˆ»çš„å¯†åº¦
                    active_ratio = density_grid.update(
                        model, 
                        device=device, 
                        time=t_val.view(1, 1), 
                        decay=1.0  # å®Œå…¨ä¿ç•™
                    )
                
                model.train()

            if step % log_every == 0:
                psnr = compute_psnr(loss_rgb_val)
                current_lr = scheduler.get_last_lr()[0]
                skip_info = ""
                if density_grid is not None:
                    skip_info = f" | Skip: {(1-active_ratio)*100:.1f}%"
                
                print(
                    f">>> Step {step}/{train_iters} | "
                    f"Loss {total_loss_val:.6f} | "
                    f"PSNR {psnr:.2f} dB | "
                    f"LR {current_lr:.6f}{skip_info}"
                )
                
                # è®°å½•åˆ° TensorBoard
                tb_logger.log_scalar('Train/RGB_Loss', loss_rgb_val, step)
                tb_logger.log_scalar('Train/Reg_Loss', loss_reg_val, step)
                tb_logger.log_scalar('Train/Total_Loss', total_loss_val, step)
                tb_logger.log_scalar('Train/PSNR', psnr, step)
                tb_logger.log_scalar('Train/LearningRate', current_lr, step)
                if use_tv_loss:
                    tb_logger.log_scalar('Train/TV_Loss', loss_tv_val, step)
                if use_temporal_smooth:
                    tb_logger.log_scalar('Train/Temporal_Loss', loss_temporal_val, step)
                if use_unsup_consistency:
                    tb_logger.log_scalar('Train/Unsup_Loss', loss_unsup_val, step)
                if density_grid is not None:
                    tb_logger.log_scalar('Train/ActiveRatio', active_ratio, step)
            
            # å®šæœŸéªŒè¯é›†è¯„ä¼°
            val_every = cfg.get("val_every", 500)
            if step % val_every == 0:
                model.eval()
                val_psnrs = []
                val_results = []  # ä¿å­˜ (idx, psnr, pred_img, time) ç”¨äºåç»­ä¿å­˜
                
                # å¯¹å…¨éƒ¨éªŒè¯é›†è®¡ç®— PSNRï¼Œéšæœºä¿å­˜ 5 å¼ å›¾ç‰‡
                import random
                n_save_images = min(5, len(val_set.images))
                save_indices = set(random.sample(range(len(val_set.images)), n_save_images))
                
                step_val_dir = os.path.join(val_render_dir, f"step_{step:06d}")
                os.makedirs(step_val_dir, exist_ok=True)
                
                with torch.no_grad():
                    # éªŒè¯æ—¶ä½¿ç”¨å›ºå®šç™½è‰²èƒŒæ™¯ï¼ˆä¿è¯å…¬å¹³å¯¹æ¯”ï¼‰
                    val_bg_color = torch.ones(3, device=device) if white_bkgd else torch.zeros(3, device=device)
                    
                    # å¯¹å…¨éƒ¨éªŒè¯é›†è®¡ç®— PSNR
                    for idx in range(len(val_set.images)):
                        rays_o, rays_d, target, time = val_set.get_image_rays(idx, device)
                        H, W = rays_o.shape[:2]
                        rays_o = rays_o.reshape(-1, 3)
                        rays_d = rays_d.reshape(-1, 3)
                        target_flat = target.reshape(-1, 3)
                        time = time.expand(H*W, 1)
                        
                        # åˆ†å—æ¸²æŸ“éªŒè¯é›†
                        pred_chunks = []
                        for i in range(0, rays_o.shape[0], chunk):
                            pred_chunk, _, _, _ = render_rays(
                                model=model,
                                rays_o=rays_o[i:i+chunk],
                                rays_d=rays_d[i:i+chunk],
                                near=near,
                                far=far,
                                n_samples=render_n_samples,
                                perturb=False,
                                times=time[i:i+chunk],
                                density_grid=density_grid,
                                bg_color=val_bg_color,  # å›ºå®šèƒŒæ™¯è‰²
                            )
                            pred_chunks.append(pred_chunk.cpu())  # ç«‹å³ç§»åŠ¨åˆ° CPU
                        pred = torch.cat(pred_chunks, dim=0)
                        del pred_chunks  # ç«‹å³é‡Šæ”¾
                        
                        val_psnr = compute_psnr_torch(pred.to(device), target_flat)
                        val_psnrs.append(val_psnr)
                        
                        # åªä¿å­˜éšæœºé€‰ä¸­çš„å›¾ç‰‡
                        if idx in save_indices:
                            pred_img = pred.reshape(H, W, 3)
                            pred_img = torch.clamp(pred_img, 0.0, 1.0)
                            plt.imsave(
                                os.path.join(step_val_dir, f"val_{idx:03d}_t{time[0,0].item():.2f}_psnr{val_psnr:.2f}.png"),
                                pred_img.numpy(),
                            )
                        del pred, target_flat, rays_o, rays_d  # æ¸…ç†æ˜¾å­˜
                
                avg_val_psnr = float(np.mean(val_psnrs))
                print(f"    [Validation] PSNR: {avg_val_psnr:.2f} dB", end="")
                
                # è®°å½•éªŒè¯é›† PSNR åˆ° TensorBoard
                tb_logger.log_scalar('Validation/PSNR', avg_val_psnr, step)
                
                plt.close('all')
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                # åªåœ¨éªŒè¯é›†PSNRæå‡æ—¶ä¿å­˜æ¨¡å‹
                if avg_val_psnr > best_val_psnr:
                    best_val_psnr = avg_val_psnr
                    best_path = os.path.join(ckpt_dir, f"best_model.pth")
                    save_dict = {
                        "model_state_dict": model.state_dict(),
                        "config": cfg,
                        "step": step,
                        "val_psnr": best_val_psnr
                    }
                    if density_grid is not None:
                        save_dict["density_grid"] = density_grid.state_dict()
                    torch.save(save_dict, best_path)
                    print(f" | ğŸŒŸ New Best Model! Saved to {best_path}")
                else:
                    print()
                
                model.train()

        print(f"\n>>> è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›† PSNR: {best_val_psnr:.2f} dB")
        tb_logger.close()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # è¯„ä¼°é˜¶æ®µ
    import shutil
    import subprocess
    import json
    from scipy.spatial.transform import Rotation, Slerp
    from scipy.interpolate import interp1d
    
    # æ¸…ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆåªä¿ç•™æµ‹è¯•é›†ç”¨äºè¯„ä¼°ï¼‰
    del train_set, val_set
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    model.eval()
    
    # è¯„ä¼°æ—¶ä½¿ç”¨å›ºå®šèƒŒæ™¯è‰²
    eval_bg_color = torch.ones(3, device=device) if white_bkgd else torch.zeros(3, device=device)
    
    # åˆ›å»ºä¸´æ—¶å›¾ç‰‡ç›®å½•ç”¨äºç”Ÿæˆè§†é¢‘
    picture_dir = os.path.join(log_dir, "picture")
    os.makedirs(picture_dir, exist_ok=True)
    
    # render_n == -1 æ—¶ï¼šç¯ç»•æ¸²æŸ“è§†é¢‘
    if render_n == -1:
        # ä»é…ç½®æ–‡ä»¶è¯»å–è§†é¢‘å‚æ•°
        n_interp_frames = cfg.get('video_frames', 300)  # è§†é¢‘æ€»å¸§æ•°
        n_rotations = cfg.get('n_rotations', 2)  # æ—‹è½¬åœˆæ•°
        print(f">>> ç¯ç»•æ¸²æŸ“æ¨¡å¼: ç”Ÿæˆ {n_interp_frames} å¸§ï¼Œç›¸æœºç»•ç‰©ä½“æ—‹è½¬ {n_rotations} åœˆï¼Œæ—¶é—´ 0â†’1...")
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–ç›¸æœºå‚æ•°
        radius = cfg.get('camera_radius', 2.4)  # ç›¸æœºç¯ç»•åŠå¾„
        
        # åœºæ™¯ä¸­å¿ƒå’Œç›¸æœºé«˜åº¦
        scene_center = cfg.get('scene_center', [0.0, 0.0, 0.0])
        camera_height = cfg.get('camera_height', 2.8)
        center = np.array(scene_center)
        
        print(f">>> ç¯ç»•åŠå¾„: {radius:.3f}")
        print(f">>> åœºæ™¯ä¸­å¿ƒ: [{center[0]:.2f}, {center[1]:.2f}, {center[2]:.2f}]")
        print(f">>> ç›¸æœºé«˜åº¦: {camera_height:.2f}")
        
        # æ—¶é—´ä» 0 çº¿æ€§å¢é•¿åˆ° 1
        interp_times = np.linspace(0.0, 1.0, n_interp_frames)
        
        # ç›¸æœºç»• Z è½´æ—‹è½¬ n_rotations åœˆ (0 åˆ° n_rotations Ã— 2Ï€)
        angles = np.linspace(0.0, n_rotations * 2 * np.pi, n_interp_frames, endpoint=False)
        
        # ç”Ÿæˆç¯ç»•ç›¸æœºä½å§¿
        interp_poses = np.zeros((n_interp_frames, 4, 4), dtype=np.float32)
        for i, angle in enumerate(angles):
            # ç›¸æœºä½ç½®ï¼šåœ¨ XY å¹³é¢ä¸Šç»•åœºæ™¯ä¸­å¿ƒæ—‹è½¬
            x = center[0] + radius * np.cos(angle)
            y = center[1] + radius * np.sin(angle)
            z = camera_height  # ä¿æŒæ’å®šé«˜åº¦
            cam_pos = np.array([x, y, z])
            
            # ç›¸æœºæœå‘åœºæ™¯ä¸­å¿ƒï¼ˆlook-atï¼‰
            forward = center - cam_pos
            forward = forward / np.linalg.norm(forward)
            
            # ä¸–ç•Œåæ ‡ç³»çš„ä¸Šæ–¹å‘
            world_up = np.array([0.0, 0.0, 1.0])
            right = np.cross(forward, world_up)
            right = right / (np.linalg.norm(right) + 1e-8)
            up = np.cross(right, forward)
            up = up / np.linalg.norm(up)
            
            # æ„å»ºæ—‹è½¬çŸ©é˜µ (NeRF ç›¸æœºåæ ‡ç³»: x=right, y=up, z=-forward)
            R = np.stack([right, up, -forward], axis=1)  # [3, 3]
            
            interp_poses[i, :3, :3] = R
            interp_poses[i, :3, 3] = cam_pos
            interp_poses[i, 3, 3] = 1.0
        
        # æ¸²æŸ“æ’å€¼å¸§
        H, W = test_set.H, test_set.W
        focal = test_set.focal
        
        with torch.no_grad():
            for idx in tqdm(range(n_interp_frames), desc="Interpolated Rendering"):
                # æ„å»ºå…‰çº¿
                c2w = torch.tensor(interp_poses[idx], dtype=torch.float32, device=device)
                t = torch.tensor([[interp_times[idx]]], dtype=torch.float32, device=device)
                
                # ç”Ÿæˆå…‰çº¿ï¼ˆä¸ dataset ä¸­ç›¸åŒçš„é€»è¾‘ï¼‰
                j, i = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')
                dirs = torch.stack([
                    (i - W * 0.5) / focal,
                    -(j - H * 0.5) / focal,
                    -torch.ones_like(i),
                ], dim=-1).reshape(-1, 3)
                
                rays_d = torch.matmul(dirs, c2w[:3, :3].T)
                rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)
                rays_o = c2w[:3, 3].expand_as(rays_d)
                if test_set.scene_scale != 1.0:
                    rays_o = rays_o * test_set.scene_scale
                
                time_batch = t.expand(H*W, 1)
                
                # åˆ†å—æ¸²æŸ“
                pred_chunks = []
                for i in range(0, rays_o.shape[0], chunk):
                    pred_chunk, _, _, _ = render_rays(
                        model=model,
                        rays_o=rays_o[i:i+chunk],
                        rays_d=rays_d[i:i+chunk],
                        near=near,
                        far=far,
                        n_samples=render_n_samples,
                        perturb=False,
                        times=time_batch[i:i+chunk],
                        density_grid=density_grid,
                        bg_color=eval_bg_color,
                    )
                    pred_chunks.append(pred_chunk)
                
                pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                pred = torch.clamp(pred, 0.0, 1.0)
                
                plt.imsave(
                    os.path.join(picture_dir, f"frame_{idx:03d}.png"),
                    pred.cpu().numpy(),
                )
                
                # æ¸…ç†æ˜¾å­˜é˜²æ­¢æ³„æ¼
                del pred, pred_chunks, rays_o, rays_d, time_batch, c2w, t, dirs
                torch.cuda.empty_cache()
        
        print(f">>> æ’å€¼æ¸²æŸ“å®Œæˆï¼å…± {n_interp_frames} å¸§")
        psnrs = []  # æ’å€¼æ¨¡å¼æ²¡æœ‰ ground truthï¼Œæ— æ³•è®¡ç®— PSNR
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šæ¸²æŸ“æŒ‡å®šæ•°é‡çš„æµ‹è¯•é›†å¸§
        print(f">>> Rendering {test_split} set...")
        psnrs = []
        num_renders = min(render_n, len(test_set))
        
        with torch.no_grad():
            for idx in tqdm(range(num_renders), desc="Rendering"):
                rays_o, rays_d, target, time = test_set.get_image_rays(idx, device)
                H, W = rays_o.shape[:2]
                rays_o = rays_o.reshape(-1, 3)
                rays_d = rays_d.reshape(-1, 3)
                time = time.expand(H*W, 1)

                pred_chunks = []
                for i in range(0, rays_o.shape[0], chunk):
                    pred_chunk, _, _, _ = render_rays(
                        model=model,
                        rays_o=rays_o[i:i+chunk],
                        rays_d=rays_d[i:i+chunk],
                        near=near,
                        far=far,
                        n_samples=render_n_samples,
                        perturb=False,
                        times=time[i:i+chunk],
                        density_grid=density_grid,
                        bg_color=eval_bg_color,
                    )
                    pred_chunks.append(pred_chunk)
                
                pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                pred = torch.clamp(pred, 0.0, 1.0)
                psnr = compute_psnr_torch(pred, target)
                psnrs.append(psnr)
                
                # ä¿å­˜ä¸ºè¿ç»­ç¼–å·çš„å¸§ï¼ˆç”¨äºç”Ÿæˆè§†é¢‘ï¼‰
                plt.imsave(
                    os.path.join(picture_dir, f"frame_{idx:03d}.png"),
                    pred.cpu().numpy(),
                )
                # åŒæ—¶ä¿å­˜å¸¦æ—¶é—´æˆ³çš„ç‰ˆæœ¬
                plt.imsave(
                    os.path.join(render_dir, f"{test_split}_{idx:03d}_t{time[0,0].item():.2f}.png"),
                    pred.cpu().numpy(),
                )
                
                del pred, pred_chunks, rays_o, rays_d, target, time
                torch.cuda.empty_cache()
        
        num_frames = num_renders

    avg_psnr = float(np.mean(psnrs)) if psnrs else 0.0
    if psnrs:
        print(f"\n>>> Test PSNR: {avg_psnr:.2f} dB")
    print(f">>> Rendered images saved to: {picture_dir}")
    
    # ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘
    dataset_name = os.path.basename(args.data_dir)
    video_path = os.path.join(log_dir, f"{dataset_name}_24fps.mp4")
    print(f"\n>>> ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘...")
    try:
        cmd = [
            "ffmpeg", "-y",
            "-framerate", "24",
            "-i", os.path.join(picture_dir, "frame_%03d.png"),
            "-c:v", "libx264",
            "-pix_fmt", "yuv420p",
            "-crf", "18",
            video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f">>> è§†é¢‘å·²ä¿å­˜: {video_path}")
            print(f">>> è§†é¢‘æ—¶é•¿: {n_interp_frames/24:.1f}ç§’ ({n_interp_frames} å¸§ @ 24fps)")
            
            # åˆ é™¤ä¸´æ—¶å›¾ç‰‡ç›®å½•
            shutil.rmtree(picture_dir)
            print(f">>> å·²æ¸…ç†ä¸´æ—¶å›¾ç‰‡ç›®å½•")
        else:
            print(f"!!! ffmpeg æ‰§è¡Œå¤±è´¥:\n{result.stderr}")
    except FileNotFoundError:
        print("!!! æœªæ‰¾åˆ° ffmpegï¼Œè¯·å…ˆå®‰è£…: sudo apt install ffmpeg")
    except Exception as e:
        print(f"!!! è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")


def run_part4(cfg, args):
    """
    Part 4: Dual-Hash Dynamic NeRF (åˆ›æ–°ç‚¹ï¼šå“ˆå¸Œä½ç§»åœº + å“ˆå¸Œè§„èŒƒåœº)
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. Dual-Hash ååŒæ¶æ„ï¼šç”¨ HashGrid æ›¿ä»£ MLP å˜å½¢ç½‘ç»œ
    2. TV-Displacement Lossï¼šå¯¹ä½ç§»ç½‘æ ¼æ–½åŠ å…¨å˜åˆ†æ­£åˆ™åŒ–
    3. æ—¶ç©ºè§£è€¦è®¾è®¡ï¼šç©ºé—´ä½ç§»ç”± HashGrid æŸ¥è¯¢ï¼Œæ—¶é—´è°ƒåˆ¶ç”±è½»é‡ MLP å®Œæˆ
    """
    if not args.data_dir:
        raise ValueError("Part 4 requires --data_dir pointing to a dynamic NeRF dataset root.")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f">>> ä½¿ç”¨è®¾å¤‡: {device}")
    print(">>> Part 4: Dual-Hash Dynamic NeRF")

    # è¯»å–æ¸²æŸ“å’Œè®­ç»ƒé…ç½®
    downscale = cfg.get("downscale", 1)
    white_bkgd = cfg.get("white_bkgd", True)
    scene_scale = cfg.get("scene_scale", 1.0)
    near = float(cfg.get("near", 2.0))
    far = float(cfg.get("far", 6.0))
    n_samples = cfg.get("n_samples", 64)
    render_n_samples = cfg.get("render_n_samples", n_samples)
    batch_size = cfg.get("batch_size", 4096)
    train_iters = cfg.get("train_iters", 20000)
    learning_rate = cfg.get("learning_rate", 5e-4)
    log_every = cfg.get("log_every", 100)
    chunk = cfg.get("chunk", 8192)
    render_n = args.render_n
    if args.render_chunk is not None:
        chunk = args.render_chunk
    log_dir = cfg.get("log_dir", "output/part4")
    
    # è·å–æ•°æ®é›†åç§°å¹¶æ·»åŠ åˆ°è¾“å‡ºè·¯å¾„
    dataset_name = os.path.basename(args.data_dir)
    log_dir = os.path.join(log_dir, dataset_name)

    os.makedirs(log_dir, exist_ok=True)
    ckpt_dir = os.path.join(log_dir)
    render_dir = os.path.join(log_dir, "renders")
    val_render_dir = os.path.join(log_dir, "val_renders")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(render_dir, exist_ok=True)
    os.makedirs(val_render_dir, exist_ok=True)

    from src.dataset import DynamicDataset
    
    train_set = DynamicDataset(
        root_dir=args.data_dir,
        split="train",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    val_set = DynamicDataset(
        root_dir=args.data_dir,
        split="val",
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    test_split = "test"
    test_meta = os.path.join(args.data_dir, "transforms_test.json")
    if not os.path.exists(test_meta):
        test_split = "val"
    test_set = DynamicDataset(
        root_dir=args.data_dir,
        split=test_split,
        downscale=downscale,
        white_bkgd=white_bkgd,
        scene_scale=scene_scale,
    )
    
    if not args.eval_only:
        print(f">>> æ•°æ®é›†: è®­ç»ƒé›† {len(train_set.images)} å¼  | éªŒè¯é›† {len(val_set.images)} å¼  | æµ‹è¯•é›† {len(test_set.images)} å¼ ")
    else:
        print(f">>> æ•°æ®é›†: æµ‹è¯•é›† {len(test_set.images)} å¼ ")

    # æ¨¡å‹åˆå§‹åŒ–
    from src.core import NeuralField
    model = NeuralField(cfg).to(device)
    
    # å¯ç”¨ density_gridï¼ˆä¸ Part 3 Instant ç›¸åŒï¼‰
    use_density_grid = cfg.get('use_density_grid', True)
    density_grid = None
    active_ratio = 1.0
    if use_density_grid:
        from src.renderer import DensityGrid
        grid_resolution = cfg.get('grid_resolution', 128)
        grid_threshold = cfg.get('grid_threshold', 0.01)
        scene_bound = cfg.get('scene_bound', 1.5)
        density_grid = DensityGrid(
            resolution=grid_resolution,
            bound=scene_bound,
            threshold=grid_threshold
        ).to(device)
        print(f">>> Density Grid å·²å¯ç”¨: {grid_resolution}Â³ åˆ†è¾¨ç‡")
    
    if args.checkpoint:
        ckpt = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        if density_grid is not None and "density_grid" in ckpt:
            density_grid.load_state_dict(ckpt["density_grid"])
            # æ›´æ–° active_ratio ä»¥åæ˜ åŠ è½½çš„ density_grid çŠ¶æ€
            active_ratio = density_grid.binary_grid.float().mean().item()
        print(f">>> Loaded checkpoint: {args.checkpoint}")

    # =========================================================================
    # è®­ç»ƒé˜¶æ®µ
    # =========================================================================
    if not args.eval_only:
        tb_dir = os.path.join(log_dir, "tensorboard", get_exp_name(cfg))
        tb_logger = TensorBoardLogger(tb_dir)
        
        weight_decay = cfg.get('weight_decay', 1e-5)
        
        # ==============================================================
        # Part 4 åˆ†ç»„å­¦ä¹ ç‡ä¼˜åŒ–ï¼ˆå…¼å®¹å•ç½‘æ ¼å’Œä¸‰ç½‘æ ¼æ¨¡å¼ï¼‰
        # ==============================================================
        param_groups = []
        
        # 1. ä¸‰ç½‘æ ¼æ¨¡å¼ï¼šåˆ†åˆ«è®¾ç½®å­¦ä¹ ç‡
        for grid_name in ['deform_grid_start', 'deform_grid_mid', 'deform_grid_end']:
            if hasattr(model, grid_name):
                grid = getattr(model, grid_name)
                param_groups.append({
                    'params': grid.parameters(),
                    'lr': learning_rate * 2.0,
                    'name': grid_name
                })
        
        # 2. å•ç½‘æ ¼æ¨¡å¼å…¼å®¹ï¼ˆå¦‚æœæ²¡æœ‰ä¸‰ç½‘æ ¼ï¼Œä½¿ç”¨ deformation_gridï¼‰
        if not hasattr(model, 'deform_grid_start') and hasattr(model, 'deformation_grid'):
            param_groups.append({
                'params': model.deformation_grid.parameters(),
                'lr': learning_rate * 2.0,
                'name': 'deformation_grid'
            })
        
        # 3. è§„èŒƒç©ºé—´å“ˆå¸Œç½‘æ ¼ï¼šé«˜å­¦ä¹ ç‡
        if hasattr(model, 'canonical_repr'):
            param_groups.append({
                'params': model.canonical_repr.parameters(),
                'lr': learning_rate * 2.0,  # 2x åŸºç¡€å­¦ä¹ ç‡
                'name': 'canonical_repr'
            })
        
        # 3. displacement_scaleï¼šè¶…é«˜å­¦ä¹ ç‡ï¼ˆæ ‡é‡å‚æ•°å­¦ä¹ æ…¢ï¼‰
        if hasattr(model, 'deform_decoder'):
            param_groups.append({
                'params': [model.deform_decoder.displacement_scale],
                'lr': learning_rate * 5.0,  # 5x åŸºç¡€å­¦ä¹ ç‡
                'name': 'displacement_scale'
            })
            # deform_net ç”¨æ­£å¸¸å­¦ä¹ ç‡
            param_groups.append({
                'params': [p for n, p in model.deform_decoder.named_parameters() if 'displacement_scale' not in n],
                'lr': learning_rate,
                'name': 'deform_decoder'
            })
        
        # 4. å…¶ä»–å‚æ•°ï¼ˆæ—¶é—´è°ƒåˆ¶ã€è§£ç å™¨ç­‰ï¼‰ï¼šæ­£å¸¸å­¦ä¹ ç‡
        excluded_names = {'deform_grid_start', 'deform_grid_mid', 'deform_grid_end', 
                         'deformation_grid', 'canonical_repr', 'deform_decoder'}
        other_params = [p for n, p in model.named_parameters() 
                       if not any(ex in n for ex in excluded_names)]
        if other_params:
            param_groups.append({
                'params': other_params,
                'lr': learning_rate,
                'name': 'others'
            })
        
        optimizer = optim.AdamW(param_groups, weight_decay=weight_decay)
        
        eta_min = cfg.get('eta_min', 1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_iters, eta_min=eta_min)
        
        use_amp = cfg.get('use_amp', True)
        scaler = torch.amp.GradScaler('cuda', enabled=use_amp)
        
        loss_fn = nn.MSELoss()
        
        # ==============================================================================
        # æ­£åˆ™åŒ–é…ç½®
        # ==============================================================================
        
        # 1. TV-Displacement Lossï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
        # å¯¹ä½ç§»å“ˆå¸Œç½‘æ ¼æ–½åŠ å…¨å˜åˆ†æ­£åˆ™åŒ–ï¼Œå¼ºåˆ¶ç›¸é‚»æ ¼ç‚¹ä½ç§»ä¸€è‡´
        use_tv_displacement = cfg.get('use_tv_displacement', True)
        tv_displacement_weight = cfg.get('tv_displacement_weight', 0.001)
        
        # 2. è§„èŒƒç©ºé—´ TV Loss
        tv_loss_weight = cfg.get('tv_loss_weight', 1e-5)
        
        # 3. å˜å½¢åœº L2 æ­£åˆ™åŒ–
        deformation_reg_weight = cfg.get('deformation_reg_weight', 0.01)
        
        # 4. æ—¶é—´å¹³æ»‘æ­£åˆ™åŒ–
        use_temporal_smooth = cfg.get('use_temporal_smooth', True)
        temporal_smooth_weight = cfg.get('temporal_smooth_weight', 1e-4)
        temporal_epsilon = cfg.get('temporal_epsilon', 0.02)
        temporal_n_samples = cfg.get('temporal_n_samples', 256)
        
        # 5. éšæœºèƒŒæ™¯å¢å¼º
        use_random_bg = cfg.get('use_random_bg', False)
        random_bg_start = cfg.get('random_bg_start', 0) if use_random_bg else float('inf')
        
        # 6. æ— ç›‘ç£ä¸€è‡´æ€§çº¦æŸ
        use_unsup_consistency = cfg.get('use_unsupervised_consistency', False)
        unsup_consistency_weight = cfg.get('unsup_consistency_weight', 0.001)
        unsup_n_samples = cfg.get('unsup_n_samples', 512)
        
        # 7. é™æ€é”šç‚¹æŸå¤±ï¼ˆå¼ºåˆ¶ t=0 æ—¶é›¶ä½ç§»ï¼‰
        use_static_anchor = cfg.get('use_static_anchor', True)
        static_anchor_weight = cfg.get('static_anchor_weight', 0.01)
        static_anchor_n_samples = cfg.get('static_anchor_n_samples', 512)
        
        # æ‰“å°é…ç½®
        print(">>> å¼€å§‹è®­ç»ƒ Part 4 (Dual-Hash Dynamic NeRF)...")
        print(f">>> tensorboard --logdir={os.path.join(log_dir, 'tensorboard')} æŸ¥çœ‹æ—¥å¿—")
        if use_tv_displacement:
            print(f">>> æ­£åˆ™åŒ–: TV-Displacement Loss å·²å¯ç”¨ (weight={tv_displacement_weight:.0e})")
        if tv_loss_weight > 0:
            print(f">>> æ­£åˆ™åŒ–: è§„èŒƒç©ºé—´ TV Loss (weight={tv_loss_weight:.0e})")
        if use_temporal_smooth:
            print(f">>> æ­£åˆ™åŒ–: æ—¶é—´å¹³æ»‘ (weight={temporal_smooth_weight:.0e}, Îµ={temporal_epsilon})")
        if use_static_anchor:
            print(f">>> æ­£åˆ™åŒ–: é™æ€é”šç‚¹æŸå¤±å·²å¯ç”¨ (weight={static_anchor_weight:.0e}, t=0 æ—¶å¼ºåˆ¶é›¶ä½ç§»)")
        if use_random_bg:
            print(f">>> æ•°æ®å¢å¼º: éšæœºèƒŒæ™¯ ({random_bg_start} æ­¥åå¼€å¯)")
        if cfg.get('use_coord_noise', False):
            print(f">>> æ•°æ®å¢å¼º: åæ ‡å™ªå£° (coord={cfg.get('coord_noise_std', 0.005)}, time={cfg.get('time_noise_std', 0.02)})")
        
        best_val_psnr = 0.0
        model.train()
        grid_update_interval = cfg.get('grid_update_interval', 32)
        grid_warmup_iters = cfg.get('grid_warmup_iters', 256)
        
        for step in range(1, train_iters + 1):
            rays_o, rays_d, target_rgba, times = train_set.sample_random_rays(batch_size, device)
            
            target_rgb = target_rgba[:, :3]
            target_alpha = target_rgba[:, 3:4]
            
            # éšæœºèƒŒæ™¯
            if use_random_bg and step >= random_bg_start:
                bg_color = torch.rand(3, device=device)
            else:
                bg_color = torch.ones(3, device=device) if white_bkgd else torch.zeros(3, device=device)
            
            target = target_rgb * target_alpha + bg_color * (1.0 - target_alpha)
            
            with torch.amp.autocast('cuda', enabled=use_amp):
                pred_rgb, _, _, extras = render_rays(
                    model=model,
                    rays_o=rays_o,
                    rays_d=rays_d,
                    near=near,
                    far=far,
                    n_samples=n_samples,
                    perturb=True,
                    times=times,
                    density_grid=density_grid,
                    bg_color=bg_color,
                )
                
                # A. RGB Loss
                loss_rgb = loss_fn(pred_rgb, target)
                
                # B. å˜å½¢åœº L2 æ­£åˆ™åŒ–
                mean_delta_x = extras['mean_delta_x']
                loss_reg = torch.mean(mean_delta_x ** 2) * deformation_reg_weight
                

                #  TV-Displacement Loss
                # å¯¹ä½ç§»å“ˆå¸Œç½‘æ ¼çš„å‚æ•°æ–½åŠ å…¨å˜åˆ†æ­£åˆ™åŒ–ï¼Œå¼ºåˆ¶ç›¸é‚»å“ˆå¸Œæ¡ç›®çš„ä½ç§»å‘é‡ç›¸ä¼¼ï¼Œæ¶ˆé™¤è¾¹ç¼˜é—ªçƒ
                loss_tv_disp = torch.tensor(0.0, device=device)
                if use_tv_displacement:
                    # ä¸‰ç½‘æ ¼ TV Lossï¼šå¯¹ä¸‰ä¸ªé”šç‚¹ç½‘æ ¼åˆ†åˆ«æ–½åŠ  TV æ­£åˆ™åŒ–
                    tv_total = 0.0
                    for grid_name in ['deform_grid_start', 'deform_grid_mid', 'deform_grid_end']:
                        if hasattr(model, grid_name):
                            grid = getattr(model, grid_name)
                            params = grid.encoding.params
                            tv_diff = torch.abs(params[1:] - params[:-1])
                            tv_total = tv_total + torch.mean(tv_diff)
                    loss_tv_disp = tv_total * tv_displacement_weight / 3.0  # å¹³å‡
                
                # D. è§„èŒƒç©ºé—´ TV Loss
                loss_tv_canon = torch.tensor(0.0, device=device)
                if tv_loss_weight > 0 and hasattr(model, 'canonical_repr'):
                    canon_params = model.canonical_repr.encoding.params
                    tv_diff_canon = torch.abs(canon_params[1:] - canon_params[:-1])
                    loss_tv_canon = torch.mean(tv_diff_canon) * tv_loss_weight
                
                # E. æ—¶é—´å¹³æ»‘æ­£åˆ™åŒ–ï¼ˆæ¯ 16 æ­¥è®¡ç®—ä¸€æ¬¡ï¼Œå¤§å¹…å‡å°‘å¼€é”€ï¼‰
                loss_temporal = torch.tensor(0.0, device=device)
                if use_temporal_smooth and step > grid_warmup_iters and step % 16 == 0:
                    n_temp = 64  # å‡å°‘é‡‡æ ·ç‚¹æ•°
                    scene_bound = cfg.get('scene_bound', 1.5)
                    
                    x_temp = (torch.rand(n_temp, 3, device=device) * 2 - 1) * scene_bound
                    t_temp = torch.rand(n_temp, 1, device=device) * (1.0 - temporal_epsilon)
                    t_temp_eps = t_temp + temporal_epsilon
                    
                    # Part 4 ä½¿ç”¨å“ˆå¸Œä½ç§»åœº
                    feat_t = model.time_encoder(t_temp)
                    feat_t_eps = model.time_encoder(t_temp_eps)
                    time_mod = model.time_modulation(feat_t)
                    time_mod_eps = model.time_modulation(feat_t_eps)
                    
                    deform_feat = model.deformation_grid(x_temp)
                    delta_x_t = model.deform_decoder(deform_feat, time_mod)
                    delta_x_t_eps = model.deform_decoder(deform_feat, time_mod_eps)
                    
                    loss_temporal = torch.mean((delta_x_t - delta_x_t_eps) ** 2) * temporal_smooth_weight * 16  # è¡¥å¿é‡‡æ ·é¢‘ç‡
                
                # F. æ— ç›‘ç£ä¸€è‡´æ€§çº¦æŸï¼ˆæ¯ 32 æ­¥è®¡ç®—ä¸€æ¬¡ï¼‰
                loss_unsup = torch.tensor(0.0, device=device)
                if use_unsup_consistency and step > grid_warmup_iters and step % 32 == 0:
                    n_unsup = 128  # å‡å°‘é‡‡æ ·ç‚¹æ•°
                    t_rand = torch.rand(n_unsup, 1, device=device)
                    scene_bound = cfg.get('scene_bound', 1.5)
                    x_rand = (torch.rand(n_unsup, 3, device=device) * 2 - 1) * scene_bound
                    
                    feat_t_rand = model.time_encoder(t_rand)
                    time_mod_rand = model.time_modulation(feat_t_rand)
                    deform_feat_rand = model.deformation_grid(x_rand)
                    delta_x_rand = model.deform_decoder(deform_feat_rand, time_mod_rand)
                    
                    loss_unsup = torch.mean(torch.abs(delta_x_rand.mean(dim=0))) * unsup_consistency_weight * 32  # è¡¥å¿é‡‡æ ·é¢‘ç‡
                
                # ==============================================================
                # â­ ä¸‰ç½‘æ ¼é”šç‚¹çº¦æŸ (Tri-Grid Anchor Loss)
                # å¯¹ä¸‰ä¸ªç½‘æ ¼åœ¨å„è‡ªçš„é”šç‚¹æ—¶åˆ»æ–½åŠ çº¦æŸï¼š
                #   - Grid_start: t=0 æ—¶å¼ºåˆ¶é›¶ä½ç§»ï¼ˆå®šä¹‰è§„èŒƒç©ºé—´ï¼‰
                #   - Grid_mid:   t=1/2 æ—¶ä½ç§»åº”å¹³æ»‘è¿ç»­
                #   - Grid_end:   t=1 æ—¶æ— ç‰¹æ®Šçº¦æŸï¼ˆéå¾ªç¯åœºæ™¯ï¼‰
                # ==============================================================
                loss_anchor = torch.tensor(0.0, device=device)
                if use_static_anchor and step > grid_warmup_iters and step % 16 == 0:
                    n_anchor = 128
                    scene_bound = cfg.get('scene_bound', 1.5)
                    
                    # éšæœºé‡‡æ ·ç©ºé—´ç‚¹
                    x_anchor = (torch.rand(n_anchor, 3, device=device) * 2 - 1) * scene_bound
                    
                    # ======== 1. t=0 å¼ºåˆ¶é›¶ä½ç§»ï¼ˆæ ¸å¿ƒçº¦æŸï¼‰========
                    # t=0 è½åœ¨ [0, 1/6] æ®µï¼Œ100% ä½¿ç”¨ Grid_start
                    t_zero = torch.zeros(n_anchor, 1, device=device)
                    feat_t_zero = model.time_encoder(t_zero)
                    time_mod_zero = model.time_modulation(feat_t_zero)
                    deform_feat_start = model.deform_grid_start(x_anchor)
                    delta_x_zero = model.deform_decoder(deform_feat_start, time_mod_zero)
                    loss_start = torch.mean(delta_x_zero ** 2)
                    
                    # ======== 2. ä¸‰ç½‘æ ¼ä¸€è‡´æ€§çº¦æŸï¼ˆå¯é€‰ï¼‰========
                    # è®©ä¸‰ä¸ªç½‘æ ¼åœ¨ t=1/6 æ—¶åˆ»è¾“å‡ºç›¸è¿‘çš„ç‰¹å¾ï¼Œç¡®ä¿æ’å€¼è¿‡æ¸¡å¹³æ»‘
                    # è¿™æ˜¯è½¯çº¦æŸï¼Œæƒé‡è¾ƒå°
                    t_anchor = torch.full((n_anchor, 1), 1.0/6.0, device=device)
                    feat_t_anchor = model.time_encoder(t_anchor)
                    time_mod_anchor = model.time_modulation(feat_t_anchor)
                    
                    # åœ¨ t=1/6 æ—¶ï¼Œstart å’Œ mid ç½‘æ ¼åº”è¯¥æœ‰ç›¸ä¼¼çš„"è¶‹åŠ¿"
                    delta_start_anchor = model.deform_decoder(model.deform_grid_start(x_anchor), time_mod_anchor)
                    delta_mid_anchor = model.deform_decoder(model.deform_grid_mid(x_anchor), time_mod_anchor)
                    # è½¯çº¦æŸï¼šä¸¤ä¸ªç½‘æ ¼åœ¨è¾¹ç•Œæ—¶åˆ»çš„è¾“å‡ºå·®å¼‚ä¸è¦å¤ªå¤§
                    loss_consistency = torch.mean((delta_start_anchor - delta_mid_anchor) ** 2) * 0.1
                    
                    # æ€»é”šç‚¹æŸå¤±
                    loss_anchor = (loss_start + loss_consistency) * static_anchor_weight * 16
                
                total_loss = loss_rgb + loss_reg + loss_tv_disp + loss_tv_canon + loss_temporal + loss_unsup + loss_anchor

            optimizer.zero_grad()
            scaler.scale(total_loss).backward()
            
            max_grad_norm = cfg.get('max_grad_norm', 1.0)
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
            
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            
            # åˆ†ç¦» loss å€¼
            loss_rgb_val = loss_rgb.item()
            loss_reg_val = loss_reg.item()
            loss_tv_disp_val = loss_tv_disp.item() if use_tv_displacement else 0.0
            loss_tv_canon_val = loss_tv_canon.item() if tv_loss_weight > 0 else 0.0
            loss_temporal_val = loss_temporal.item() if use_temporal_smooth else 0.0
            loss_unsup_val = loss_unsup.item() if use_unsup_consistency else 0.0
            loss_anchor_val = loss_anchor.item() if use_static_anchor else 0.0
            total_loss_val = total_loss.item()
            
            del extras
            
            # åŠ¨æ€ç½‘æ ¼æ›´æ–°
            if step < train_iters * 0.1:
                dynamic_interval = 16
            elif step < train_iters * 0.5:
                dynamic_interval = 64
            else:
                dynamic_interval = 256
            
            grid_stop_ratio = cfg.get('grid_stop_ratio', 0.9)
            if density_grid is not None and step < train_iters * grid_stop_ratio and density_grid.should_update(step, dynamic_interval, grid_warmup_iters):
                model.eval()
                # â­ ä¸‰ç½‘æ ¼æ¶æ„ï¼šåªéœ€é‡‡æ ·ä¸‰ä¸ªé”šç‚¹æ—¶åˆ»å³å¯è¦†ç›–è¿åŠ¨è½¨è¿¹
                # é¿å…è¿‡åº¦é‡‡æ ·å¯¼è‡´ Skip ç‡æš´è·Œ
                anchor_times = torch.tensor([1.0/6.0, 0.5, 5.0/6.0], device=device)
                
                # æ¯ 500 æ­¥å¯ç”¨è‡ªåŠ¨å‰ªæï¼Œé¿å… Skip ç‡æš´è·Œ
                enable_prune = (step % 500 == 0) and (step > grid_warmup_iters)
                
                for t_val in anchor_times:
                    active_ratio = density_grid.update(
                        model, device=device, time=t_val.view(1, 1), decay=1.0,
                        auto_prune=enable_prune, threshold_multiplier=1.0
                    )
                model.train()

            if step % log_every == 0:
                psnr = compute_psnr(loss_rgb_val)
                current_lr = scheduler.get_last_lr()[0]
                skip_info = f" | Skip: {(1-active_ratio)*100:.1f}%" if density_grid else ""
                
                print(
                    f">>> Step {step}/{train_iters} | "
                    f"Loss {total_loss_val:.6f} | "
                    f"PSNR {psnr:.2f} dB | "
                    f"LR {current_lr:.6f}{skip_info}"
                )
                
                tb_logger.log_scalar('Train/RGB_Loss', loss_rgb_val, step)
                tb_logger.log_scalar('Train/Reg_Loss', loss_reg_val, step)
                tb_logger.log_scalar('Train/Total_Loss', total_loss_val, step)
                tb_logger.log_scalar('Train/PSNR', psnr, step)
                tb_logger.log_scalar('Train/LearningRate', current_lr, step)
                if use_tv_displacement:
                    tb_logger.log_scalar('Train/TV_Displacement_Loss', loss_tv_disp_val, step)
                if tv_loss_weight > 0:
                    tb_logger.log_scalar('Train/TV_Canon_Loss', loss_tv_canon_val, step)
                if use_temporal_smooth:
                    tb_logger.log_scalar('Train/Temporal_Loss', loss_temporal_val, step)
                if use_unsup_consistency:
                    tb_logger.log_scalar('Train/Unsup_Loss', loss_unsup_val, step)
                if use_static_anchor:
                    tb_logger.log_scalar('Train/Anchor_Loss', loss_anchor_val, step)
                if density_grid is not None:
                    tb_logger.log_scalar('Train/ActiveRatio', active_ratio, step)
            
            # éªŒè¯é›†è¯„ä¼°
            val_every = cfg.get("val_every", 500)
            if step % val_every == 0:
                model.eval()
                val_psnrs = []
                
                import random
                n_save_images = min(5, len(val_set.images))
                save_indices = set(random.sample(range(len(val_set.images)), n_save_images))
                
                step_val_dir = os.path.join(val_render_dir, f"step_{step:06d}")
                os.makedirs(step_val_dir, exist_ok=True)
                
                with torch.no_grad():
                    val_bg_color = torch.ones(3, device=device) if white_bkgd else torch.zeros(3, device=device)
                    
                    for idx in range(len(val_set.images)):
                        rays_o, rays_d, target, time = val_set.get_image_rays(idx, device)
                        H, W = rays_o.shape[:2]
                        rays_o = rays_o.reshape(-1, 3)
                        rays_d = rays_d.reshape(-1, 3)
                        target_flat = target.reshape(-1, 3)
                        time = time.expand(H*W, 1)
                        
                        pred_chunks = []
                        for i in range(0, rays_o.shape[0], chunk):
                            pred_chunk, _, _, _ = render_rays(
                                model=model,
                                rays_o=rays_o[i:i+chunk],
                                rays_d=rays_d[i:i+chunk],
                                near=near,
                                far=far,
                                n_samples=render_n_samples,
                                perturb=False,
                                times=time[i:i+chunk],
                                density_grid=density_grid,
                                bg_color=val_bg_color,
                            )
                            pred_chunks.append(pred_chunk.cpu())
                        pred = torch.cat(pred_chunks, dim=0)
                        del pred_chunks
                        
                        val_psnr = compute_psnr_torch(pred.to(device), target_flat)
                        val_psnrs.append(val_psnr)
                        
                        if idx in save_indices:
                            pred_img = pred.reshape(H, W, 3)
                            pred_img = torch.clamp(pred_img, 0.0, 1.0)
                            plt.imsave(
                                os.path.join(step_val_dir, f"val_{idx:03d}_t{time[0,0].item():.2f}_psnr{val_psnr:.2f}.png"),
                                pred_img.numpy(),
                            )
                        del pred, target_flat, rays_o, rays_d
                
                avg_val_psnr = float(np.mean(val_psnrs))
                print(f"    [Validation] PSNR: {avg_val_psnr:.2f} dB", end="")
                
                tb_logger.log_scalar('Validation/PSNR', avg_val_psnr, step)
                
                plt.close('all')
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                if avg_val_psnr > best_val_psnr:
                    best_val_psnr = avg_val_psnr
                    best_path = os.path.join(ckpt_dir, f"best_model.pth")
                    save_dict = {
                        "model_state_dict": model.state_dict(),
                        "config": cfg,
                        "step": step,
                        "val_psnr": best_val_psnr
                    }
                    if density_grid is not None:
                        save_dict["density_grid"] = density_grid.state_dict()
                    torch.save(save_dict, best_path)
                    print(f" | ğŸŒŸ New Best! Saved to {best_path}")
                else:
                    print()
                
                model.train()

        print(f"\n>>> è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›† PSNR: {best_val_psnr:.2f} dB")
        tb_logger.close()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # =========================================================================
    # è¯„ä¼°é˜¶æ®µ
    # =========================================================================
    import shutil
    import subprocess
    from scipy.spatial.transform import Rotation, Slerp
    from scipy.interpolate import interp1d
    
    del train_set, val_set
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    model.eval()
    eval_bg_color = torch.ones(3, device=device) if white_bkgd else torch.zeros(3, device=device)
    
    # eval_only æ¨¡å¼ï¼šåªæµ‹è¯• PSNRï¼Œä¸ç”Ÿæˆè§†é¢‘
    if args.eval_only:
        print(f"\n>>> è¯„ä¼°æ¨¡å¼ï¼šè®¡ç®—æµ‹è¯•é›† PSNR...")
        psnrs = []
        with torch.no_grad():
            for idx in tqdm(range(len(test_set)), desc="Evaluating"):
                rays_o, rays_d, target, time = test_set.get_image_rays(idx, device)
                H, W = rays_o.shape[:2]
                rays_o = rays_o.reshape(-1, 3)
                rays_d = rays_d.reshape(-1, 3)
                time = time.expand(H*W, 1)

                pred_chunks = []
                for i in range(0, rays_o.shape[0], chunk):
                    pred_chunk, _, _, _ = render_rays(
                        model=model,
                        rays_o=rays_o[i:i+chunk],
                        rays_d=rays_d[i:i+chunk],
                        near=near,
                        far=far,
                        n_samples=render_n_samples,
                        perturb=False,
                        times=time[i:i+chunk],
                        density_grid=density_grid,
                        bg_color=eval_bg_color,
                    )
                    pred_chunks.append(pred_chunk)
                
                pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                pred = torch.clamp(pred, 0.0, 1.0)
                psnr = compute_psnr_torch(pred, target)
                psnrs.append(psnr)
                
                del pred, pred_chunks, rays_o, rays_d, target, time
                torch.cuda.empty_cache()
        
        avg_psnr = float(np.mean(psnrs))
        print(f"\n{'='*60}")
        print(f">>> Part 4 æµ‹è¯•é›†è¯„ä¼°ç»“æœ")
        print(f">>> å¹³å‡ PSNR: {avg_psnr:.2f} dB ({len(psnrs)} å¼ å›¾ç‰‡)")
        print(f"{'='*60}")
        return
    
    # è®­ç»ƒæ¨¡å¼ç»“æŸåç›´æ¥è¿”å›ï¼Œä¸ç”Ÿæˆè§†é¢‘
    if not args.eval_only:
        print(f"\n>>> è®­ç»ƒå®Œæˆï¼ä½¿ç”¨ --eval_only --render_n -1 æ¥ç”Ÿæˆè§†é¢‘")
        return
    
    # --eval_only + render_n != -1ï¼šæ¸²æŸ“æŒ‡å®šæ•°é‡çš„æµ‹è¯•é›†å›¾ç‰‡
    # --eval_only + render_n == -1ï¼šç”Ÿæˆç¯ç»•è§†é¢‘
    picture_dir = os.path.join(log_dir, "picture")
    os.makedirs(picture_dir, exist_ok=True)
    
    if render_n == -1:
        n_interp_frames = cfg.get('video_frames', 300)
        n_rotations = cfg.get('n_rotations', 2)
        print(f">>> ç¯ç»•æ¸²æŸ“æ¨¡å¼: ç”Ÿæˆ {n_interp_frames} å¸§ï¼Œç›¸æœºç»•ç‰©ä½“æ—‹è½¬ {n_rotations} åœˆ...")
        
        radius = cfg.get('camera_radius', 2.4)
        scene_center = cfg.get('scene_center', [0.0, 0.0, 0.0])
        camera_height = cfg.get('camera_height', 2.8)
        center = np.array(scene_center)
        
        print(f">>> ç¯ç»•åŠå¾„: {radius:.3f}, åœºæ™¯ä¸­å¿ƒ: {center}, ç›¸æœºé«˜åº¦: {camera_height:.2f}")
        
        interp_times = np.linspace(0.0, 1.0, n_interp_frames)
        angles = np.linspace(0.0, n_rotations * 2 * np.pi, n_interp_frames, endpoint=False)
        
        interp_poses = np.zeros((n_interp_frames, 4, 4), dtype=np.float32)
        for i, angle in enumerate(angles):
            x = center[0] + radius * np.cos(angle)
            y = center[1] + radius * np.sin(angle)
            z = camera_height
            cam_pos = np.array([x, y, z])
            
            forward = center - cam_pos
            forward = forward / np.linalg.norm(forward)
            world_up = np.array([0.0, 0.0, 1.0])
            right = np.cross(forward, world_up)
            right = right / (np.linalg.norm(right) + 1e-8)
            up = np.cross(right, forward)
            up = up / np.linalg.norm(up)
            
            R = np.stack([right, up, -forward], axis=1)
            interp_poses[i, :3, :3] = R
            interp_poses[i, :3, 3] = cam_pos
            interp_poses[i, 3, 3] = 1.0
        
        H, W = test_set.H, test_set.W
        focal = test_set.focal
        
        with torch.no_grad():
            for idx in tqdm(range(n_interp_frames), desc="Rendering"):
                c2w = torch.tensor(interp_poses[idx], dtype=torch.float32, device=device)
                t = torch.tensor([[interp_times[idx]]], dtype=torch.float32, device=device)
                
                j, i = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')
                dirs = torch.stack([
                    (i - W * 0.5) / focal,
                    -(j - H * 0.5) / focal,
                    -torch.ones_like(i),
                ], dim=-1).reshape(-1, 3)
                
                rays_d = torch.matmul(dirs, c2w[:3, :3].T)
                rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)
                rays_o = c2w[:3, 3].expand_as(rays_d)
                if test_set.scene_scale != 1.0:
                    rays_o = rays_o * test_set.scene_scale
                
                time_batch = t.expand(H*W, 1)
                
                pred_chunks = []
                for i in range(0, rays_o.shape[0], chunk):
                    pred_chunk, _, _, _ = render_rays(
                        model=model,
                        rays_o=rays_o[i:i+chunk],
                        rays_d=rays_d[i:i+chunk],
                        near=near,
                        far=far,
                        n_samples=render_n_samples,
                        perturb=False,
                        times=time_batch[i:i+chunk],
                        density_grid=density_grid,
                        bg_color=eval_bg_color,
                    )
                    pred_chunks.append(pred_chunk)
                
                pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                pred = torch.clamp(pred, 0.0, 1.0)
                
                plt.imsave(
                    os.path.join(picture_dir, f"frame_{idx:03d}.png"),
                    pred.cpu().numpy(),
                )
                
                del pred, pred_chunks, rays_o, rays_d, time_batch, c2w, t, dirs
                torch.cuda.empty_cache()
        
        print(f">>> æ¸²æŸ“å®Œæˆï¼å…± {n_interp_frames} å¸§")
        psnrs = []
    else:
        print(f">>> Rendering {test_split} set...")
        psnrs = []
        num_renders = min(render_n, len(test_set))
        
        with torch.no_grad():
            for idx in tqdm(range(num_renders), desc="Rendering"):
                rays_o, rays_d, target, time = test_set.get_image_rays(idx, device)
                H, W = rays_o.shape[:2]
                rays_o = rays_o.reshape(-1, 3)
                rays_d = rays_d.reshape(-1, 3)
                time = time.expand(H*W, 1)

                pred_chunks = []
                for i in range(0, rays_o.shape[0], chunk):
                    pred_chunk, _, _, _ = render_rays(
                        model=model,
                        rays_o=rays_o[i:i+chunk],
                        rays_d=rays_d[i:i+chunk],
                        near=near,
                        far=far,
                        n_samples=render_n_samples,
                        perturb=False,
                        times=time[i:i+chunk],
                        density_grid=density_grid,
                        bg_color=eval_bg_color,
                    )
                    pred_chunks.append(pred_chunk)
                
                pred = torch.cat(pred_chunks, dim=0).reshape(H, W, 3)
                pred = torch.clamp(pred, 0.0, 1.0)
                psnr = compute_psnr_torch(pred, target)
                psnrs.append(psnr)
                
                plt.imsave(os.path.join(picture_dir, f"frame_{idx:03d}.png"), pred.cpu().numpy())
                plt.imsave(os.path.join(render_dir, f"{test_split}_{idx:03d}_t{time[0,0].item():.2f}.png"), pred.cpu().numpy())
                
                del pred, pred_chunks, rays_o, rays_d, target, time
                torch.cuda.empty_cache()
        
        n_interp_frames = num_renders

    avg_psnr = float(np.mean(psnrs)) if psnrs else 0.0
    if psnrs:
        print(f"\n>>> Test PSNR: {avg_psnr:.2f} dB")
    print(f">>> Rendered images saved to: {picture_dir}")
    
    # ç”Ÿæˆè§†é¢‘
    dataset_name = os.path.basename(args.data_dir)
    video_path = os.path.join(log_dir, f"{dataset_name}_part4_24fps.mp4")
    print(f"\n>>> ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘...")
    try:
        cmd = [
            "ffmpeg", "-y",
            "-framerate", "24",
            "-i", os.path.join(picture_dir, "frame_%03d.png"),
            "-c:v", "libx264",
            "-pix_fmt", "yuv420p",
            "-crf", "18",
            video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f">>> è§†é¢‘å·²ä¿å­˜: {video_path}")
            print(f">>> è§†é¢‘æ—¶é•¿: {n_interp_frames/24:.1f}ç§’ ({n_interp_frames} å¸§ @ 24fps)")
            shutil.rmtree(picture_dir)
            print(f">>> å·²æ¸…ç†ä¸´æ—¶å›¾ç‰‡ç›®å½•")
        else:
            print(f"!!! ffmpeg æ‰§è¡Œå¤±è´¥:\n{result.stderr}")
    except FileNotFoundError:
        print("!!! æœªæ‰¾åˆ° ffmpegï¼Œè¯·å…ˆå®‰è£…: sudo apt install ffmpeg")
    except Exception as e:
        print(f"!!! è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--image", type=str, help="è¾“å…¥å›¾åƒè·¯å¾„ (Part 1)")
    parser.add_argument("--data_dir", type=str, help="NeRF æ•°æ®é›†æ ¹ç›®å½• (Part 2)")
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", type=str, help="åŠ è½½å·²è®­ç»ƒæ¨¡å‹")
    parser.add_argument(
        "--eval_only",
        action="store_true",
        help="ä»…è¯„ä¼°/æ¸²æŸ“ï¼Œä¸è¿›è¡Œè®­ç»ƒï¼ˆéœ€ --checkpointï¼‰",
    )
    parser.add_argument("--render_n", type=int, default=-1, help="è¯„ä¼°æ—¶æ¸²æŸ“çš„æµ‹è¯•é›†å›¾ç‰‡æ•°é‡ï¼Œå¦‚æœä¸º -1 åˆ™æ’å€¼æ¸²æŸ“ 300 å¸§") 
    parser.add_argument("--render_chunk", type=int, help="è¦†ç›–æ¸²æŸ“ chunk å¤§å°")
    args = parser.parse_args()

    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    mode = cfg.get("mode")
    if mode == "part1_fourier":
        if not args.image:
            raise ValueError("Part 1 requires --image.")
        if args.eval_only and not args.checkpoint:
            raise ValueError("Part 1 eval_only requires --checkpoint.")
        run_part1(cfg, args)
    elif mode == "part2_nerf":
        if args.eval_only and not args.checkpoint:
            raise ValueError("eval_only requires --checkpoint.")
        run_part2(cfg, args)
    elif mode == "part2_instant":
        if args.eval_only and not args.checkpoint:
            raise ValueError("eval_only requires --checkpoint.")
        run_part2_instant(cfg, args)
    elif mode == "part3":
        if args.eval_only and not args.checkpoint:
            raise ValueError("eval_only requires --checkpoint.")
        run_part3(cfg, args)
    elif mode == "part4":
        if args.eval_only and not args.checkpoint:
            raise ValueError("eval_only requires --checkpoint.")
        run_part4(cfg, args)
    else:
        raise ValueError(f"Unsupported mode: {mode}")
